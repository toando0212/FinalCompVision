{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:29.967088Z",
     "start_time": "2025-04-09T04:06:29.947030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# === KHÔNG THAY ĐỔI (Imports cơ bản và thêm thư viện cho HTR) ===\n",
    "import os\n",
    "import math # === THÊM: Thêm math để tính toán output width nếu cần ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import sns # Không cần thiết cho HTR cơ bản\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models # <--- THAY ĐỔI: Thêm models\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import matplotlib.pyplot as plt # Thêm để vẽ đồ thị loss (tùy chọn)\n",
    "import json # Import json để lưu/tải map ký tự\n",
    "\n",
    "# === THÊM: Kiểm tra và cảnh báo tương thích checkpoint ===\n",
    "EXPECTED_ARCHITECTURE = \"EfficientNetB0_CRNN\" # Đặt tên cho kiến trúc mới"
   ],
   "id": "f04663922ccd9a9",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:30.045923Z",
     "start_time": "2025-04-09T04:06:30.030920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === KHÔNG THAY ĐỔI (Thiết lập đường dẫn) ===\n",
    "# Đường dẫn thư mục dữ liệu\n",
    "data_dir = r\"E:\\HANDS-VNOnDB\" # <--- KIỂM TRA LẠI ĐƯỜNG DẪN NÀY\n",
    "label_file_path = os.path.join(data_dir, \"normalized_InkData_line.csv\") # <--- File chứa cả ID và label text\n",
    "image_folder_original = os.path.join(data_dir, \"InkData_line\")\n",
    "\n",
    "# Thư mục lưu ảnh đã xử lý (resize)\n",
    "image_folder_processed = r\"D:\\HANDS-VNOnDB\\Processed_Images_Lines_HTR2\" "
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:30.076285Z",
     "start_time": "2025-04-09T04:06:30.053084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Chuẩn hóa file gán nhãn\n",
    "\n",
    "# def split_id(df, id_col=\"id\"):\n",
    "#     df[['document_id', 'line_id', 'word_id']] = df[id_col].str.rsplit('_', n=2, expand=True)\n",
    "#     return df.head()\n",
    "\n",
    "# print((pd.read_csv(os.path.join(data_dir, \"normalized_InkData_line.csv\"))))"
   ],
   "id": "42300f39aa46ee87",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:30.123269Z",
     "start_time": "2025-04-09T04:06:30.109269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TARGET_IMG_HEIGHT = 32\n",
    "TARGET_IMG_WIDTH = 384 "
   ],
   "id": "1740783711f78574",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:30.168706Z",
     "start_time": "2025-04-09T04:06:30.156012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_image_htr(image_path, target_size=(TARGET_IMG_WIDTH, TARGET_IMG_HEIGHT)):\n",
    "    \"\"\"Mở ảnh, chuyển grayscale, resize.\"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img_gray = img.convert('L')\n",
    "            img_resized = img_gray.resize(target_size, Image.Resampling.LANCZOS)\n",
    "        return img_resized\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi mở hoặc xử lý ảnh {image_path}: {e}\")\n",
    "        return None"
   ],
   "id": "ca3d7a094f2fb46a",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:30.200499Z",
     "start_time": "2025-04-09T04:06:30.185791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_and_save_images_htr(input_folder, output_folder, target_size=(TARGET_IMG_WIDTH, TARGET_IMG_HEIGHT)):\n",
    "    \"\"\"Tiền xử lý và lưu ảnh vào thư mục mới.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Đã tạo thư mục: {output_folder}\")\n",
    "\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.endswith(('.png', '.jpg'))]\n",
    "    print(f\"Tìm thấy {len(image_files)} ảnh trong {input_folder}\")\n",
    "\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    error_count = 0\n",
    "    for img_file in tqdm(image_files, desc=\"Preprocessing images for HTR\", unit=\"image\"):\n",
    "        img_path = os.path.join(input_folder, img_file)\n",
    "        output_img_path = os.path.join(output_folder, img_file)\n",
    "\n",
    "        if not os.path.exists(output_img_path):\n",
    "            processed_img = preprocess_image_htr(img_path, target_size)\n",
    "            if processed_img:\n",
    "                try:\n",
    "                    processed_img.save(output_img_path)\n",
    "                    processed_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Lỗi khi lưu ảnh {output_img_path}: {e}\")\n",
    "                    error_count += 1\n",
    "            else:\n",
    "                 error_count += 1 # Lỗi trong hàm preprocess_image_htr\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "\n",
    "    total_processed_or_skipped = processed_count + skipped_count\n",
    "    print(f\"Hoàn thành tiền xử lý:\")\n",
    "    print(f\"- Đã xử lý mới: {processed_count}\")\n",
    "    print(f\"- Bỏ qua (đã tồn tại): {skipped_count}\")\n",
    "    print(f\"- Lỗi: {error_count}\")\n",
    "    print(f\"- Tổng số ảnh trong thư mục output: {total_processed_or_skipped}\")\n",
    "    print(f\"Ảnh đã xử lý được lưu tại: {output_folder}\")"
   ],
   "id": "1331ea1f6a01dc93",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:30.653701Z",
     "start_time": "2025-04-09T04:06:30.217502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chạy tiền xử lý ảnh (chỉ chạy nếu thư mục processed chưa có hoặc muốn ghi đè)\n",
    "run_preprocessing = True # Đặt thành False nếu đã chạy và không muốn chạy lại\n",
    "if run_preprocessing:\n",
    "    process_and_save_images_htr(image_folder_original, image_folder_processed, target_size=(TARGET_IMG_WIDTH, TARGET_IMG_HEIGHT))\n",
    "else:\n",
    "    print(f\"Bỏ qua bước tiền xử lý ảnh, sử dụng ảnh tại: {image_folder_processed}\")\n",
    "    if not os.path.exists(image_folder_processed):\n",
    "        print(f\"CẢNH BÁO: Thư mục ảnh đã xử lý {image_folder_processed} không tồn tại nhưng run_preprocessing=False.\")\n",
    "        exit()\n"
   ],
   "id": "7d8b68ce2342949c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tìm thấy 7296 ảnh trong E:\\HANDS-VNOnDB\\InkData_line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing images for HTR: 100%|██████████| 7296/7296 [00:00<00:00, 17240.32image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoàn thành tiền xử lý:\n",
      "- Đã xử lý mới: 0\n",
      "- Bỏ qua (đã tồn tại): 7296\n",
      "- Lỗi: 0\n",
      "- Tổng số ảnh trong thư mục output: 7296\n",
      "Ảnh đã xử lý được lưu tại: D:\\HANDS-VNOnDB\\Processed_Images_Lines_HTR2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:30.747582Z",
     "start_time": "2025-04-09T04:06:30.716564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === THAY ĐỔI (Load nhãn TEXT thay vì ID) ===\n",
    "# Đọc file CSV chứa nhãn\n",
    "try:\n",
    "    labels_df = pd.read_csv(label_file_path)\n",
    "    print(f\"Đã đọc thành công file label: {label_file_path}\")\n",
    "    # print(\"5 dòng đầu tiên của labels_df:\")\n",
    "    # print(labels_df.head())\n",
    "    if 'label' not in labels_df.columns or 'id' not in labels_df.columns:\n",
    "        raise ValueError(\"File CSV phải chứa cột 'id' và 'label'.\")\n",
    "    # Xử lý giá trị NaN trong cột label (thay bằng chuỗi rỗng hoặc bỏ qua)\n",
    "    labels_df['label'] = labels_df['label'].fillna('')\n",
    "    print(f\"Số lượng dòng trong file label: {len(labels_df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"LỖI khi đọc hoặc kiểm tra file CSV: {e}\")\n",
    "    exit()"
   ],
   "id": "f07e1eaf28983abe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã đọc thành công file label: E:\\HANDS-VNOnDB\\normalized_InkData_line.csv\n",
      "Số lượng dòng trong file label: 7296\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:30.810107Z",
     "start_time": "2025-04-09T04:06:30.795110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "def normalize_text(text):\n",
    "    \"\"\"Loại bỏ ký tự đặc biệt, chuyển về chữ thường, bỏ khoảng trắng thừa.\"\"\"\n",
    "    text = str(text).lower()  # Chắc chắn là string và chuyển về chữ thường\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Loại bỏ ký tự đặc biệt\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Loại bỏ khoảng trắng thừa\n",
    "    return text"
   ],
   "id": "ec54442dd84f64d0",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:30.842126Z",
     "start_time": "2025-04-09T04:06:30.827110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Hàm tải đường dẫn ảnh và nhãn TEXT, đảm bảo khớp ID và ảnh tồn tại\n",
    "def load_image_paths_and_text_labels(processed_image_folder, labels_df):\n",
    "    image_paths = []\n",
    "    text_labels = []\n",
    "    missing_images = 0\n",
    "    valid_ids_from_df = set(labels_df['id'])\n",
    "    processed_image_files = os.listdir(processed_image_folder)\n",
    "    processed_ids = set(os.path.splitext(f)[0] for f in processed_image_files if f.endswith(('.png', '.jpg')))\n",
    "\n",
    "    print(f\"Số ID hợp lệ từ CSV: {len(valid_ids_from_df)}\")\n",
    "    print(f\"Số ảnh tìm thấy trong thư mục đã xử lý: {len(processed_ids)}\")\n",
    "\n",
    "    # Lọc DataFrame để chỉ giữ lại các ID có ảnh tương ứng\n",
    "    labels_df_filtered = labels_df[labels_df['id'].isin(processed_ids)].copy()\n",
    "    print(f\"Số lượng mẫu sau khi lọc (có cả ảnh và label): {len(labels_df_filtered)}\")\n",
    "    \n",
    "    text_labels = [normalize_text(label) for label in text_labels]\n",
    "\n",
    "    # Tạo dictionary từ ID sang label text (từ DataFrame đã lọc)\n",
    "    id_to_label = pd.Series(labels_df_filtered.label.values, index=labels_df_filtered.id).to_dict()\n",
    "\n",
    "    # Lặp qua các ID đã lọc để tạo list cuối cùng\n",
    "    for img_id in tqdm(labels_df_filtered['id'], desc=\"Khớp ảnh và nhãn\"):\n",
    "        img_filename = f\"{img_id}.png\" # Hoặc .jpg nếu có\n",
    "        img_path = os.path.join(processed_image_folder, img_filename)\n",
    "        # Kiểm tra lại lần nữa (dù đã lọc)\n",
    "        if os.path.exists(img_path):\n",
    "            image_paths.append(img_path)\n",
    "            text_labels.append(id_to_label[img_id])\n",
    "        else:\n",
    "             missing_images += 1 # Không nên xảy ra nếu lọc đúng\n",
    "\n",
    "    if missing_images > 0:\n",
    "        print(f\"Cảnh báo: {missing_images} ảnh có trong df_filtered nhưng không tìm thấy file!\")\n",
    "        print(\"\\nDEBUG: Ví dụ về nhãn trước khi mã hóa:\")\n",
    "    for i in range(min(30, len(text_labels))):  # In 5 mẫu đầu tiên\n",
    "        print(f\"  - Mẫu {i}: '{text_labels[i]}' (độ dài: {len(text_labels[i])})\")\n",
    "\n",
    "    return image_paths, text_labels"
   ],
   "id": "b00c6339cd0d289f",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:31.228695Z",
     "start_time": "2025-04-09T04:06:30.874208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tải dữ liệu\n",
    "all_image_paths, all_text_labels = load_image_paths_and_text_labels(image_folder_processed, labels_df)\n",
    "\n",
    "if not all_image_paths:\n",
    "    print(\"Lỗi: Không tải được ảnh hoặc nhãn nào. Kiểm tra lại đường dẫn và file.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Đã tải thành công {len(all_image_paths)} cặp ảnh-nhãn.\")\n",
    "print(\"Ví dụ 5 nhãn đầu tiên:\", all_text_labels[:5])"
   ],
   "id": "62395cacb5ef0d5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số ID hợp lệ từ CSV: 7296\n",
      "Số ảnh tìm thấy trong thư mục đã xử lý: 7296\n",
      "Số lượng mẫu sau khi lọc (có cả ảnh và label): 7296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Khớp ảnh và nhãn: 100%|██████████| 7296/7296 [00:00<00:00, 22687.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mẫu 0: 'Bản chất của thành công' (độ dài: 23)\n",
      "  - Mẫu 1: 'Đã bao giờ bạn tự hỏi thành công là gì mà bao kẻ bỏ cả cuộc đời mình theo đuổi? Phải' (độ dài: 84)\n",
      "  - Mẫu 2: 'chăng đó là kết quả hoàn hảo trong công việc, sự chính xác đến từng chi tiết? Hay đó' (độ dài: 84)\n",
      "  - Mẫu 3: 'là cách nói khác của từ thành đạt, nghĩa là có được một cuộc sống giàu sang, được' (độ dài: 81)\n",
      "  - Mẫu 4: 'mọi người nể phục? Vậy thì bạn hãy dành chút thời gian để lặng mình suy ngẫm.' (độ dài: 77)\n",
      "  - Mẫu 5: 'Cuộc sống sẽ chỉ cho bạn có những người đạt được thành công theo một' (độ dài: 68)\n",
      "  - Mẫu 6: 'cách giản dị đến bất ngờ.' (độ dài: 25)\n",
      "  - Mẫu 7: 'Thành công là khi bố và con trai có dũng khí bước vào bếp, nấu những món ăn mẹ' (độ dài: 78)\n",
      "  - Mẫu 8: 'thích nhân ngày 8 - 3. Món canh có thể hơi mặn, món cá sốt đáng lẽ phải có màu' (độ dài: 78)\n",
      "  - Mẫu 9: 'đỏ sậm thì lại ngả sang màu... đen cháy. Nhưng nhìn mâm cơm, mẹ vẫn cười.' (độ dài: 73)\n",
      "  - Mẫu 10: 'Bởi vì hai bố con không thể thành công trên \" chiến trường \" bếp núc, nhưng lại thành công' (độ dài: 90)\n",
      "  - Mẫu 11: 'khi tặng mẹ \" đoá hồng \" của tình yêu. Một món quà ý nghĩa hơn cả những món' (độ dài: 75)\n",
      "  - Mẫu 12: 'quà quý giá, hạnh phúc ấy long lanh in trong mắt mẹ.' (độ dài: 52)\n",
      "  - Mẫu 13: 'Thành công còn là hình ảnh một cậu bé bị dị tật ở chân, không bao giờ đi lại bình' (độ dài: 81)\n",
      "  - Mẫu 14: 'thường được. Từ nhỏ cậu đã nuôi ước mơ trở thành cầu thủ bóng đá. Sau bao nỗ lực' (độ dài: 80)\n",
      "  - Mẫu 15: 'khổ luyện, cậu bé trở thành cầu thủ dự bị trong một đội bóng nhỏ, và chưa bao' (độ dài: 77)\n",
      "  - Mẫu 16: 'giờ được chính thức ra sân. Nhưng đó không phải là thất bại. Trái lại, thành công' (độ dài: 81)\n",
      "  - Mẫu 17: 'đã nở hoa khi cậu bé năm xưa, với bao nghị lực và quyết tâm, đã chiến thắng' (độ dài: 75)\n",
      "  - Mẫu 18: 'hoàn cảnh để theo đuổi ước mơ từ ngày thơ bé.' (độ dài: 45)\n",
      "  - Mẫu 19: 'Thành công ấy, liệu có mấy người đạt được?' (độ dài: 42)\n",
      "  - Mẫu 20: 'Sau mỗi mùa thi đại học, có bao \" sĩ tử \" buồn rầu khi biết mình trở thành \" tử' (độ dài: 79)\n",
      "  - Mẫu 21: 'sĩ \". Hai bảy điểm, cao thật đấy. Nhưng cao mà làm gì khi NV1 lấy tới hai bảy' (độ dài: 77)\n",
      "  - Mẫu 22: 'phẩy năm? Đó thật ra không phải là thất bại, chỉ là khi thành công - bị - trì - hoãn mà' (độ dài: 87)\n",
      "  - Mẫu 23: 'thôi. Cuộc sống vẫn chào đón họ với NV2, NV3. Quan trọng là họ đã nỗ lực hết sức để' (độ dài: 83)\n",
      "  - Mẫu 24: 'khẳng định mình. Đó là ý nghĩa vẹn nguyên của các kỳ thi, và cũng là bản chất của' (độ dài: 81)\n",
      "  - Mẫu 25: 'thành công.' (độ dài: 11)\n",
      "  - Mẫu 26: 'KHÁI QUÁT VỀ BIỂN ĐẢO VIỆT NAM' (độ dài: 30)\n",
      "  - Mẫu 27: 'Nước ta giáp với biển Đông ở hai phía Đông và Nam. Vùng biển Việt Nam là một phần' (độ dài: 81)\n",
      "  - Mẫu 28: 'biển Đông.' (độ dài: 10)\n",
      "  - Mẫu 29: 'Bờ biển dài 3.260 km, từ Quảng Ninh đến Kiên Giang. Như vậy cứ 100 km2 thì' (độ dài: 74)\n",
      "Đã tải thành công 7296 cặp ảnh-nhãn.\n",
      "Ví dụ 5 nhãn đầu tiên: ['Bản chất của thành công', 'Đã bao giờ bạn tự hỏi thành công là gì mà bao kẻ bỏ cả cuộc đời mình theo đuổi? Phải', 'chăng đó là kết quả hoàn hảo trong công việc, sự chính xác đến từng chi tiết? Hay đó', 'là cách nói khác của từ thành đạt, nghĩa là có được một cuộc sống giàu sang, được', 'mọi người nể phục? Vậy thì bạn hãy dành chút thời gian để lặng mình suy ngẫm.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:31.305711Z",
     "start_time": "2025-04-09T04:06:31.267696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === THAY ĐỔI (Chia Train/Test cho dữ liệu HTR) ===\n",
    "# Đường dẫn lưu file train/test sau khi chia\n",
    "train_csv_path = os.path.join(data_dir, \"train_htr_lines.csv\")\n",
    "test_csv_path = os.path.join(data_dir, \"test_htr_lines.csv\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_paths, X_test_paths, y_train_text, y_test_text = train_test_split(\n",
    "    all_image_paths, all_text_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Số lượng mẫu train: {len(X_train_paths)}\")\n",
    "print(f\"Số lượng mẫu test: {len(X_test_paths)}\")\n",
    "\n",
    "try:\n",
    "    # Tạo DataFrame và lưu train set\n",
    "    train_data_to_save = pd.DataFrame({'image_path': X_train_paths, 'label': y_train_text})\n",
    "    train_data_to_save.to_csv(train_csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"Đã lưu tập train vào: {train_csv_path}\")\n",
    "\n",
    "    # Tạo DataFrame và lưu test set\n",
    "    test_data_to_save = pd.DataFrame({'image_path': X_test_paths, 'label': y_test_text})\n",
    "    test_data_to_save.to_csv(test_csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"Đã lưu tập test vào: {test_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi lưu file train/test CSV: {e}\")\n"
   ],
   "id": "53f46100bc1a5a22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng mẫu train: 5836\n",
      "Số lượng mẫu test: 1460\n",
      "Đã lưu tập train vào: E:\\HANDS-VNOnDB\\train_htr_lines.csv\n",
      "Đã lưu tập test vào: E:\\HANDS-VNOnDB\\test_htr_lines.csv\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:31.384293Z",
     "start_time": "2025-04-09T04:06:31.322701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === THAY ĐỔI (Tạo bộ ký tự và mã hóa nhãn cho HTR) ===\n",
    "\n",
    "# Tìm tất cả các ký tự duy nhất từ tập train và test\n",
    "all_texts_for_vocab = y_train_text + y_test_text\n",
    "characters = set(char for text in all_texts_for_vocab for char in str(text))\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "# Thêm ký tự đặc biệt 'blank' cho CTC Loss (thường ở index 0)\n",
    "BLANK_TOKEN = '<blank>'\n",
    "if BLANK_TOKEN not in characters:\n",
    "    characters.insert(0, BLANK_TOKEN) # Đảm bảo blank ở vị trí 0\n",
    "\n",
    "print(f\"Tổng số ký tự (bao gồm blank): {len(characters)}\")\n",
    "# print(\"Bộ ký tự:\", \"\".join(characters)) # Có thể rất dài\n",
    "\n",
    "# Tạo từ điển ánh xạ\n",
    "char_to_int = {char: i for i, char in enumerate(characters)}\n",
    "int_to_char = {i: char for i, char in enumerate(characters)}\n",
    "\n",
    "# Số lượng lớp cho mô hình = số ký tự\n",
    "num_classes_htr = len(characters)\n",
    "print(f\"Số lớp đầu ra cho mô hình HTR: {num_classes_htr}\")\n",
    "\n",
    "# Hàm mã hóa text thành chuỗi số nguyên\n",
    "def encode_text(text, char_to_int_map):\n",
    "    return [char_to_int_map[char] for char in str(text) if char in char_to_int_map]\n",
    "\n",
    "# Mã hóa nhãn train và test\n",
    "y_train_encoded = [encode_text(text, char_to_int) for text in y_train_text]\n",
    "y_test_encoded = [encode_text(text, char_to_int) for text in y_test_text]\n",
    "\n",
    "# In thử vài mẫu đã mã hóa\n",
    "print(\"\\nVí dụ mã hóa:\")\n",
    "print(\"Nhãn gốc:\", y_train_text[0])\n",
    "print(\"Nhãn mã hóa:\", y_train_encoded[0])\n",
    "print(\"Giải mã lại:\", \"\".join([int_to_char[i] for i in y_train_encoded[0]]))"
   ],
   "id": "d836a8da784db0ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số ký tự (bao gồm blank): 162\n",
      "Số lớp đầu ra cho mô hình HTR: 162\n",
      "\n",
      "Ví dụ mã hóa:\n",
      "Nhãn gốc: Ngược lại, không có việc thì chúng tôi về, không phải tự nguyện xin về\n",
      "Nhãn mã hóa: [39, 57, 107, 150, 53, 1, 62, 108, 59, 9, 1, 61, 58, 93, 64, 57, 1, 53, 92, 1, 72, 59, 133, 53, 1, 70, 58, 89, 1, 53, 58, 96, 64, 57, 1, 70, 93, 59, 1, 72, 128, 9, 1, 61, 58, 93, 64, 57, 1, 66, 58, 110, 59, 1, 70, 158, 1, 64, 57, 71, 74, 133, 64, 1, 73, 59, 64, 1, 72, 128]\n",
      "Giải mã lại: Ngược lại, không có việc thì chúng tôi về, không phải tự nguyện xin về\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:31.617029Z",
     "start_time": "2025-04-09T04:06:31.413877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- THÊM CODE LƯU FILE ĐÃ MÃ HÓA ---\n",
    "import json\n",
    "from tqdm import tqdm # Đảm bảo tqdm đã được import\n",
    "print(\"\\nBắt đầu lưu file đã mã hóa và bản đồ ký tự...\")\n",
    "\n",
    "# Định nghĩa đường dẫn file lưu (cùng thư mục với train/test csv)\n",
    "encoded_train_path = os.path.join(data_dir, \"encoded_train_labels.json\")\n",
    "encoded_test_path = os.path.join(data_dir, \"encoded_test_labels.json\")\n",
    "charmap_path = os.path.join(data_dir, \"character_map.json\")\n",
    "\n",
    "try:\n",
    "    # Lưu y_train_encoded\n",
    "    with open(encoded_train_path, 'w', encoding='utf-8') as f_train:\n",
    "        json.dump(y_train_encoded, f_train)\n",
    "    print(f\"Đã lưu nhãn train đã mã hóa vào: {encoded_train_path}\")\n",
    "\n",
    "    # Lưu y_test_encoded\n",
    "    with open(encoded_test_path, 'w', encoding='utf-8') as f_test:\n",
    "        json.dump(y_test_encoded, f_test)\n",
    "    print(f\"Đã lưu nhãn test đã mã hóa vào: {encoded_test_path}\")\n",
    "\n",
    "    # Lưu cả hai bản đồ ký tự vào một file\n",
    "    # Chuyển key của int_to_char thành string để tương thích JSON\n",
    "    int_to_char_str_keys = {str(k): v for k, v in int_to_char.items()}\n",
    "    char_maps = {\n",
    "        'char_to_int': char_to_int,\n",
    "        'int_to_char': int_to_char_str_keys # Lưu phiên bản key là string\n",
    "    }\n",
    "    with open(charmap_path, 'w', encoding='utf-8') as f_map:\n",
    "        # indent=4 để dễ đọc file JSON hơn\n",
    "        # ensure_ascii=False để lưu đúng ký tự tiếng Việt\n",
    "        json.dump(char_maps, f_map, ensure_ascii=False, indent=4)\n",
    "    print(f\"Đã lưu bản đồ ký tự vào: {charmap_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi lưu file JSON đã mã hóa: {e}\")\n",
    "# --- KẾT THÚC CODE LƯU FILE ---"
   ],
   "id": "b164a1ac6ab11e57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bắt đầu lưu file đã mã hóa và bản đồ ký tự...\n",
      "Đã lưu nhãn train đã mã hóa vào: E:\\HANDS-VNOnDB\\encoded_train_labels.json\n",
      "Đã lưu nhãn test đã mã hóa vào: E:\\HANDS-VNOnDB\\encoded_test_labels.json\n",
      "Đã lưu bản đồ ký tự vào: E:\\HANDS-VNOnDB\\character_map.json\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:31.679619Z",
     "start_time": "2025-04-09T04:06:31.665091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === THAY ĐỔI: Tính toán CNN Output Width cho EfficientNet-B0 BỊ CẮT BỚT SỚM HƠN ===\n",
    "# Cần chạy cell này TRƯỚC khi định nghĩa collate_fn và model\n",
    "\n",
    "import traceback # Import traceback for more detailed error printing\n",
    "\n",
    "# --- Helper function để lấy phần CNN bị cắt bớt SỚM HƠN ---\n",
    "def get_truncated_efficientnet_features():\n",
    "    \"\"\"Tải EfficientNet-B0 và trả về phần features bị cắt bớt RẤT SỚM (ví dụ: tới block 2).\"\"\"\n",
    "    # Tải cấu trúc (weights=None để nhanh)\n",
    "    effnet_full_features = models.efficientnet_b0(weights=None).features\n",
    "    # Chọn các block bạn muốn giữ lại. Ví dụ: block 0 đến 2 (3 block đầu tiên)\n",
    "    # [:3] tương ứng với output của stage 2 (reduction factor 4)\n",
    "    truncated_cnn = nn.Sequential(*list(effnet_full_features[:3]))\n",
    "    return truncated_cnn\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def calculate_cnn_output_width(img_height, img_width):\n",
    "    \"\"\"\n",
    "    Ước tính chiều rộng và số kênh output của CNN EfficientNet-B0 ĐÃ BỊ CẮT BỚT SỚM.\n",
    "    \"\"\"\n",
    "    # Dự đoán fallback width và channels cho [:3]\n",
    "    cnn_output_w = img_width // 4 # Ước tính width = input_width / 4\n",
    "    cnn_output_c = 40 # Output channels của block 2 trong EffNetB0\n",
    "\n",
    "    # ... (Phần try-except-finally giữ nguyên logic, chỉ thay đổi hàm gọi) ...\n",
    "    effnet_features = None\n",
    "    dummy_input = None\n",
    "    output_features = None\n",
    "\n",
    "    try:\n",
    "        print(\"Xây dựng kiến trúc CNN bị cắt bớt SỚM HƠN (ví dụ: EffNetB0 features[:3])...\")\n",
    "        truncated_cnn = get_truncated_efficientnet_features() # Gọi hàm helper mới\n",
    "        truncated_cnn.eval()\n",
    "        print(\"Xây dựng CNN bị cắt bớt thành công.\")\n",
    "\n",
    "        dummy_input = torch.randn(1, 3, img_height, img_width)\n",
    "        print(f\"Tạo dummy input shape: {dummy_input.shape}\")\n",
    "\n",
    "        print(\"Đang chạy dummy input qua CNN bị cắt bớt...\")\n",
    "        with torch.no_grad():\n",
    "            output_features = truncated_cnn(dummy_input)\n",
    "        print(\"Chạy dummy input thành công.\")\n",
    "\n",
    "        cnn_output_c = output_features.shape[1]\n",
    "        cnn_output_w = output_features.shape[3]\n",
    "\n",
    "        print(f\"--- Tính toán Output CNN (Truncated[:3] EfficientNet-B0) ---\") # Cập nhật tiêu đề\n",
    "        print(f\"Input H={img_height}, W={img_width}\")\n",
    "        print(f\"Output shape after TRUNCATED[:3] features: [N, C, H, W] = {output_features.shape}\")\n",
    "        print(f\"===> Số kênh output của CNN (C_out): {cnn_output_c}\")\n",
    "        print(f\"===> Chiều rộng output của CNN (W_out / SeqLen for RNN): {cnn_output_w}\")\n",
    "        print(f\"---------------------------------------------------------\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # ... (Phần xử lý lỗi giữ nguyên) ...\n",
    "        print(f\"\\nLỖI khi chạy dummy input qua CNN bị cắt bớt SỚM: {e}\")\n",
    "        traceback.print_exc()\n",
    "        print(f\"===> Sử dụng giá trị mặc định dự đoán: W={cnn_output_w}, C={cnn_output_c}\")\n",
    "\n",
    "    finally:\n",
    "        # ... (Phần dọn dẹp giữ nguyên) ...\n",
    "         if truncated_cnn is not None: del truncated_cnn\n",
    "         if dummy_input is not None: del dummy_input\n",
    "         if output_features is not None: del output_features\n",
    "         gc.collect()\n",
    "         if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "    # Return the calculated values\n",
    "    return cnn_output_w, cnn_output_c\n",
    "\n",
    "# --- Tính toán và gán giá trị toàn cục ---\n",
    "if 'TARGET_IMG_HEIGHT' not in globals() or 'TARGET_IMG_WIDTH' not in globals():\n",
    "     raise ValueError(\"TARGET_IMG_HEIGHT và TARGET_IMG_WIDTH chưa được định nghĩa!\")\n",
    "\n",
    "print(\"Bắt đầu tính toán CNN_OUTPUT_WIDTH và CNN_OUTPUT_CHANNELS cho CNN bị cắt bớt SỚM HƠN...\")\n",
    "CNN_OUTPUT_WIDTH, CNN_OUTPUT_CHANNELS = calculate_cnn_output_width(TARGET_IMG_HEIGHT, TARGET_IMG_WIDTH)\n",
    "# Kiểm tra xem width đã đủ lớn chưa\n",
    "if CNN_OUTPUT_WIDTH < 93: # So sánh với max label length\n",
    "     print(f\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "     print(f\"CẢNH BÁO: CNN_OUTPUT_WIDTH ({CNN_OUTPUT_WIDTH}) vẫn nhỏ hơn max label length (93).\")\n",
    "     print(f\"          CTC Loss có thể vẫn lỗi với các nhãn dài.\")\n",
    "     print(f\"          Cân nhắc tăng TARGET_IMG_WIDTH hơn nữa hoặc dùng CNN khác.\")\n",
    "     print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "else:\n",
    "     print(f\"Đã tính toán xong: CNN_OUTPUT_WIDTH = {CNN_OUTPUT_WIDTH} (>= 93), CNN_OUTPUT_CHANNELS = {CNN_OUTPUT_CHANNELS}\")\n",
    "\n",
    "\n",
    "# CẬP NHẬT TÊN KIẾN TRÚC ĐỂ LƯU CHECKPOINT\n",
    "EXPECTED_ARCHITECTURE = \"EfficientNetB0_Truncated_Early_CRNN\" # Tên mới hơn\n",
    "print(f\"Kiến trúc dự kiến cho checkpoint: {EXPECTED_ARCHITECTURE}\")"
   ],
   "id": "4109bcd47a73d1d2",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:31.710741Z",
     "start_time": "2025-04-09T04:06:31.695604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === THAY ĐỔI: Transforms cho ảnh HTR sử dụng EfficientNet ===\n",
    "# EfficientNet mong đợi ảnh 3 kênh và chuẩn hóa theo ImageNet\n",
    "\n",
    "htr_transform = transforms.Compose([\n",
    "    # Ảnh đã được resize và grayscale trong bước tiền xử lý trước đó\n",
    "    # và được load bằng PIL trong Dataset (nên là mode 'L')\n",
    "\n",
    "    # Chuyển PIL Image (mode 'L', H, W) [0-255] thành Tensor (1, H, W) [0, 1]\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Lặp lại kênh đơn thành 3 kênh để phù hợp EfficientNet input (3, H, W)\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "\n",
    "    # Chuẩn hóa ImageNet cho 3 kênh\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"\\nĐã định nghĩa htr_transform cho EfficientNet.\")"
   ],
   "id": "7e4a611ce06c35fc",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:31.758008Z",
     "start_time": "2025-04-09T04:06:31.744873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === THAY ĐỔI NHẸ: Class Dataset (Thêm xử lý lỗi và trả về None) ===\n",
    "class CustomDatasetHTR(Dataset):\n",
    "    def __init__(self, image_paths, encoded_labels, transform=None):\n",
    "        # Đảm bảo số lượng paths và labels khớp nhau\n",
    "        if len(image_paths) != len(encoded_labels):\n",
    "            raise ValueError(f\"Số lượng image_paths ({len(image_paths)}) không khớp với encoded_labels ({len(encoded_labels)})!\")\n",
    "        self.image_paths = image_paths\n",
    "        self.encoded_labels = encoded_labels # List các list số nguyên\n",
    "        self.transform = transform\n",
    "        print(f\"Khởi tạo CustomDatasetHTR với {len(self.image_paths)} mẫu.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Lấy path và label gốc (list số nguyên)\n",
    "        img_path = self.image_paths[idx]\n",
    "        label_list = self.encoded_labels[idx]\n",
    "\n",
    "        # Chuyển đổi label thành tensor và kiểm tra rỗng sớm\n",
    "        try:\n",
    "            encoded_label_tensor = torch.tensor(label_list, dtype=torch.long)\n",
    "            if len(encoded_label_tensor) == 0:\n",
    "                 # Nhãn rỗng sau mã hóa, bỏ qua sample này\n",
    "                 # print(f\"Debug: Sample {idx} ({os.path.basename(img_path)}) has empty label list. Returning None.\")\n",
    "                 return None # collate_fn sẽ lọc bỏ\n",
    "        except Exception as e:\n",
    "             print(f\"Lỗi khi chuyển label list thành tensor cho sample {idx} ({os.path.basename(img_path)}): {e}\")\n",
    "             print(f\"Label list: {label_list}\")\n",
    "             return None # collate_fn sẽ lọc bỏ\n",
    "\n",
    "        # Xử lý ảnh\n",
    "        try:\n",
    "            # Mở ảnh bằng Pillow (chế độ mặc định)\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            # Đảm bảo ảnh là Grayscale ('L') trước khi áp dụng transform\n",
    "            if image.mode != 'L':\n",
    "                 # print(f\"Debug: Converting image {os.path.basename(img_path)} from mode {image.mode} to 'L'.\")\n",
    "                 image = image.convert('L')\n",
    "\n",
    "            # Áp dụng transform (bao gồm ToTensor, repeat channels, Normalize)\n",
    "            if self.transform:\n",
    "                image_tensor = self.transform(image)\n",
    "            else:\n",
    "                 # Nếu không có transform, cần tự chuyển sang Tensor và xử lý kênh\n",
    "                 # Nên luôn có transform cho model PyTorch\n",
    "                 image_tensor = transforms.ToTensor()(image) # Chỉ chuyển thành Tensor (1, H, W)\n",
    "                 # Cần xử lý kênh nếu không có transform lặp kênh\n",
    "                 if image_tensor.shape[0] == 1:\n",
    "                      image_tensor = image_tensor.repeat(3, 1, 1) # Lặp kênh thủ công\n",
    "\n",
    "            # Trả về ảnh tensor và label tensor\n",
    "            return image_tensor, encoded_label_tensor\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             print(f\"Lỗi: Không tìm thấy file ảnh tại {img_path} cho sample {idx}.\")\n",
    "             return None # collate_fn sẽ lọc bỏ\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi tải hoặc transform ảnh {img_path} cho sample {idx}: {e}\")\n",
    "            import traceback\n",
    "            # traceback.print_exc() # Bỏ comment để xem chi tiết lỗi nếu cần\n",
    "            return None # collate_fn sẽ lọc bỏ\n",
    "        \n",
    "        print(f\"\\nDEBUG: Dataset __getitem__({idx}) - Path: {img_path}\")\n",
    "        print(f\"  - Nhãn gốc (trước khi mã hóa): {self.labels[idx]}\")\n",
    "        print(f\"  - Nhãn đã mã hóa (trước khi kiểm tra độ dài): {label_list}\")\n",
    "    \n",
    "        if len(encoded_label_tensor) > CNN_OUTPUT_WIDTH:\n",
    "            print(f\"CẢNH BÁO: Nhãn {idx} VẪN dài ({len(encoded_label_tensor)} > {CNN_OUTPUT_WIDTH}). Bỏ qua.\")\n",
    "            return None"
   ],
   "id": "b7588d6c251dbf18",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4ad53d2cdc5409cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:31.788550Z",
     "start_time": "2025-04-09T04:06:31.775535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import traceback  # Import traceback for detailed error printing\n",
    "# import gc # Import gc for garbage collection\n",
    "\n",
    "def collate_fn_htr(batch):\n",
    "    \"\"\"\n",
    "    Collates a batch of data for HTR, handling variable-length labels and images.\n",
    "    Filters out None items from the batch and ensures label lengths are compatible with input lengths.\n",
    "    \"\"\"\n",
    "    # --- 1. Filter out None items ---\n",
    "    original_batch_size = len(batch)\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    filtered_batch_size = len(batch)\n",
    "\n",
    "    if original_batch_size != filtered_batch_size:\n",
    "        print(f\"collate_fn_htr: Filtered {original_batch_size - filtered_batch_size} None items from batch.\")\n",
    "\n",
    "    if not batch:\n",
    "        print(\"collate_fn_htr: Warning - Empty batch after filtering. Returning None.\")\n",
    "        return None, None, None, None # Return None for image, label, input_lengths, label_lengths\n",
    "\n",
    "    # --- 2. Separate images and labels ---\n",
    "    try:\n",
    "        images, labels = zip(*batch)\n",
    "\n",
    "        # --- 3. Stack Images (Convert to Tensor) ---\n",
    "        print(\"collate_fn_htr: Stacking images...\")\n",
    "        images = torch.stack(images, 0)\n",
    "        print(f\"collate_fn_htr: Images shape: {images.shape}\")\n",
    "\n",
    "        # --- 4. Calculate Label Lengths (BEFORE Padding) ---\n",
    "        print(\"collate_fn_htr: Calculating label lengths...\")\n",
    "        label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long)\n",
    "        print(f\"collate_fn_htr: Initial label_lengths: {label_lengths}\")\n",
    "\n",
    "        # --- 5. Get BLANK_TOKEN Index ---\n",
    "        print(\"collate_fn_htr: Getting blank token index...\")\n",
    "        if 'char_to_int' not in globals() or 'BLANK_TOKEN' not in globals():\n",
    "            raise NameError(\"char_to_int or BLANK_TOKEN not defined. Run character map creation!\")\n",
    "        blank_token_idx = char_to_int.get(BLANK_TOKEN, 0) # Default to 0 if not found\n",
    "        print(f\"collate_fn_htr: Blank token index: {blank_token_idx}\")\n",
    "\n",
    "        # --- 6. TRUNCATE LONG LABELS (CRITICAL STEP) ---\n",
    "        print(\"collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\")\n",
    "        if 'CNN_OUTPUT_WIDTH' not in globals():\n",
    "             raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
    "\n",
    "        exceeding_indices = label_lengths > CNN_OUTPUT_WIDTH\n",
    "        if exceeding_indices.any():\n",
    "            print(\"\\nWARNING: Found labels longer than CNN_OUTPUT_WIDTH. Truncating labels!\")\n",
    "            for i in torch.where(exceeding_indices)[0]:\n",
    "              print(f\"  - Sample {i}: label_length={label_lengths[i]}, CNN_OUTPUT_WIDTH={CNN_OUTPUT_WIDTH}\")\n",
    "              labels[i] = labels[i][:CNN_OUTPUT_WIDTH] # Truncate the actual label\n",
    "              print(f\"Truncated label for sample {i}: {labels[i]}\")\n",
    "              label_lengths[i] = torch.tensor(len(labels[i]), dtype=torch.long) # Update length\n",
    "              print(f\"New label_length for sample {i}: {label_lengths[i]}\")\n",
    "\n",
    "\n",
    "        # --- 7. Pad Labels (after truncation!) ---\n",
    "        print(\"collate_fn_htr: Padding labels...\")\n",
    "        padded_labels = pad_sequence(labels, batch_first=True, padding_value=blank_token_idx)\n",
    "        print(f\"collate_fn_htr: Padded labels shape: {padded_labels.shape}\")\n",
    "\n",
    "        # --- 8. Calculate Input Lengths ---\n",
    "        print(\"collate_fn_htr: Calculating input lengths...\")\n",
    "        current_batch_size = images.size(0)\n",
    "        input_lengths = torch.full(size=(current_batch_size,), fill_value=CNN_OUTPUT_WIDTH, dtype=torch.long)\n",
    "        print(f\"collate_fn_htr: Input lengths: {input_lengths}\")\n",
    "\n",
    "        # --- 9. CHECK & Clamp VALID LENGTHS (AFTER PADDING and TRUNCATION) ---\n",
    "        print(\"collate_fn_htr: Clamping valid lengths...\")\n",
    "        valid_input_lengths = torch.clamp(input_lengths, max=CNN_OUTPUT_WIDTH)\n",
    "        valid_label_lengths = torch.clamp(label_lengths, max=CNN_OUTPUT_WIDTH)\n",
    "        print(f\"collate_fn_htr: Valid input lengths: {valid_input_lengths}\")\n",
    "        print(f\"collate_fn_htr: Valid label lengths: {valid_label_lengths}\")\n",
    "\n",
    "        # --- 10. Debug Printing (AFTER ALL TRANSFORMATIONS) ---\n",
    "        print(\"\\nDEBUG: COLLATED BATCH INFO\")\n",
    "        print(f\"  - Batch Size: {current_batch_size}\")\n",
    "        print(f\"  - Images Shape: {images.shape}\")\n",
    "        print(f\"  - Padded Labels Shape: {padded_labels.shape}\")\n",
    "        print(f\"  - Input Lengths (before clamp): {input_lengths}\")\n",
    "        print(f\"  - Valid Input Lengths (clamped): {valid_input_lengths}\")\n",
    "        print(f\"  - Valid Label Lengths (clamped): {valid_label_lengths}\")\n",
    "        print(f\"  - Max Label Length (after truncation): {valid_label_lengths.max()}\")\n",
    "\n",
    "        # --- 11. Final Check - Negative/Zero Lengths ---\n",
    "        if torch.any(label_lengths <= 0):\n",
    "            print(\"ERROR: Found labels with length <= 0 AFTER truncation. This should NOT happen!\")\n",
    "            print(f\"Label lengths: {label_lengths}\")\n",
    "            # Potentially add more robust error handling or filtering here\n",
    "        print(\"collate_fn_htr: Returning data...\")\n",
    "        return images, padded_labels, valid_input_lengths, valid_label_lengths\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR in collate_fn_htr: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None"
   ],
   "id": "462281b074d8cae3",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:31.867378Z",
     "start_time": "2025-04-09T04:06:31.805536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === KHÔNG THAY ĐỔI (Thử load dữ liệu đã mã hóa từ file JSON - tùy chọn) ===\n",
    "# Cell này chủ yếu để xác nhận file đã lưu đúng, dữ liệu chính đã được mã hóa và lọc ở các cell trên.\n",
    "\n",
    "print(\"\\nKiểm tra và thử tải lại dữ liệu đã mã hóa từ file JSON (để xác nhận)...\")\n",
    "loaded_encoded_data_check = False # Flag kiểm tra\n",
    "if os.path.exists(encoded_train_path) and \\\n",
    "   os.path.exists(encoded_test_path) and \\\n",
    "   os.path.exists(charmap_path):\n",
    "    try:\n",
    "        # Chỉ tải để kiểm tra, không ghi đè biến hiện tại trừ khi có lý do\n",
    "        with open(encoded_train_path, 'r', encoding='utf-8') as f_train:\n",
    "            _ = json.load(f_train) # Tải vào biến tạm\n",
    "        with open(encoded_test_path, 'r', encoding='utf-8') as f_test:\n",
    "            _ = json.load(f_test)\n",
    "        with open(charmap_path, 'r', encoding='utf-8') as f_map:\n",
    "            _ = json.load(f_map)\n",
    "\n",
    "        print(f\"Kiểm tra tải file JSON thành công: {encoded_train_path}, {encoded_test_path}, {charmap_path}\")\n",
    "        loaded_encoded_data_check = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi kiểm tra tải file JSON đã mã hóa: {e}. Tiếp tục với dữ liệu trong bộ nhớ.\")\n",
    "else:\n",
    "    print(\"Không tìm thấy một hoặc nhiều file JSON đã mã hóa/charmap để kiểm tra.\")"
   ],
   "id": "e094d5e9d9fe78b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kiểm tra và thử tải lại dữ liệu đã mã hóa từ file JSON (để xác nhận)...\n",
      "Kiểm tra tải file JSON thành công: E:\\HANDS-VNOnDB\\encoded_train_labels.json, E:\\HANDS-VNOnDB\\encoded_test_labels.json, E:\\HANDS-VNOnDB\\character_map.json\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:31.897654Z",
     "start_time": "2025-04-09T04:06:31.887638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === KHÔNG THAY ĐỔI (Tạo Dataset) ===\n",
    "# Tạo Dataset và DataLoader mới với dữ liệu đã lọc và transform mới\n",
    "# Đảm bảo X_train_paths, y_train_encoded, X_test_paths, y_test_encoded là các phiên bản đã lọc\n",
    "\n",
    "# Kiểm tra lại dữ liệu trước khi tạo Dataset\n",
    "if not X_train_paths or not y_train_encoded:\n",
    "     print(\"Lỗi: Không có dữ liệu huấn luyện hợp lệ sau khi lọc. Không thể tạo Dataset.\")\n",
    "     # exit()\n",
    "elif not X_test_paths or not y_test_encoded:\n",
    "     print(\"Cảnh báo: Không có dữ liệu kiểm tra hợp lệ sau khi lọc. Tập test sẽ rỗng.\")\n",
    "     # Tạo test_dataset rỗng hoặc xử lý phù hợp\n",
    "     test_dataset_htr = None # Hoặc dataset rỗng\n",
    "else:\n",
    "     print(f\"\\nSẵn sàng tạo Dataset với {len(X_train_paths)} mẫu train và {len(X_test_paths)} mẫu test.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    train_dataset_htr = CustomDatasetHTR(X_train_paths, y_train_encoded, transform=htr_transform)\n",
    "    print(f\"Đã tạo train_dataset_htr.\")\n",
    "\n",
    "    # Chỉ tạo test_dataset nếu có dữ liệu test\n",
    "    if X_test_paths and y_test_encoded:\n",
    "         test_dataset_htr = CustomDatasetHTR(X_test_paths, y_test_encoded, transform=htr_transform)\n",
    "         print(f\"Đã tạo test_dataset_htr.\")\n",
    "    else:\n",
    "         print(\"Không có dữ liệu test hợp lệ, test_dataset_htr sẽ là None.\")\n",
    "         test_dataset_htr = None\n",
    "\n",
    "except Exception as e:\n",
    "     print(f\"\\nLỗi khi khởi tạo CustomDatasetHTR: {e}\")\n",
    "     print(\"Kiểm tra lại dữ liệu đầu vào (paths, labels) và class Dataset.\")\n",
    "     # exit()"
   ],
   "id": "84a876bc0e4080a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sẵn sàng tạo Dataset với 5836 mẫu train và 1460 mẫu test.\n",
      "Khởi tạo CustomDatasetHTR với 5836 mẫu.\n",
      "Đã tạo train_dataset_htr.\n",
      "Khởi tạo CustomDatasetHTR với 1460 mẫu.\n",
      "Đã tạo test_dataset_htr.\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:31.944656Z",
     "start_time": "2025-04-09T04:06:31.925637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === KHÔNG THAY ĐỔI (Tạo DataLoader) ===\n",
    "# Điều chỉnh batch_size tùy theo bộ nhớ GPU/CPU\n",
    "BATCH_SIZE = 32 # Giữ nguyên hoặc giảm nếu gặp lỗi CUDA out of memory\n",
    "NUM_WORKERS = 0 # Đặt là 0 để dễ debug, tăng lên (ví dụ 2 hoặc 4) để tăng tốc độ nếu không có lỗi\n",
    "\n",
    "print(f\"\\nĐang tạo DataLoaders với batch_size={BATCH_SIZE}, num_workers={NUM_WORKERS}...\")\n",
    "\n",
    "# Tạo train_loader\n",
    "if 'train_dataset_htr' in globals() and train_dataset_htr is not None and len(train_dataset_htr) > 0:\n",
    "    train_loader_htr = DataLoader(\n",
    "        train_dataset_htr,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn_htr, # Sử dụng collate function đã định nghĩa\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False # Pin memory nếu dùng CUDA\n",
    "    )\n",
    "    print(f\"Đã tạo train_loader_htr với {len(train_loader_htr)} batches.\")\n",
    "else:\n",
    "     print(\"Lỗi: train_dataset_htr không hợp lệ hoặc rỗng. Không thể tạo train_loader.\")\n",
    "     train_loader_htr = None # Đặt là None để xử lý lỗi sau\n",
    "\n",
    "\n",
    "# Tạo test_loader (chỉ nếu test_dataset hợp lệ)\n",
    "if 'test_dataset_htr' in globals() and test_dataset_htr is not None and len(test_dataset_htr) > 0:\n",
    "    test_loader_htr = DataLoader(\n",
    "        test_dataset_htr,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False, # Không cần shuffle test set\n",
    "        collate_fn=collate_fn_htr,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    print(f\"Đã tạo test_loader_htr với {len(test_loader_htr)} batches.\")\n",
    "else:\n",
    "     print(\"test_dataset_htr không hợp lệ hoặc rỗng. test_loader_htr sẽ là None.\")\n",
    "     test_loader_htr = None\n",
    "\n",
    "\n"
   ],
   "id": "5e7fd7cfda4d7009",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Đang tạo DataLoaders với batch_size=32, num_workers=0...\n",
      "Đã tạo train_loader_htr với 183 batches.\n",
      "Đã tạo test_loader_htr với 46 batches.\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:07:54.699595Z",
     "start_time": "2025-04-09T04:07:54.693594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === THAY ĐỔI: Định nghĩa mô hình CRNN với EfficientNet-B0 BỊ CẮT BỚT SỚM HƠN ===\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes, rnn_hidden_size=256, rnn_num_layers=2, dropout=0.5):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        # --- Phần CNN: Sử dụng EfficientNet-B0 Pre-trained BỊ CẮT BỚT SỚM HƠN ---\n",
    "        weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "        effnet_full = models.efficientnet_b0(weights=weights)\n",
    "\n",
    "        # Xây dựng CNN bị cắt bớt SỚM HƠN (phải giống hệt cell tính toán width)\n",
    "        # Ví dụ: lấy 3 block đầu tiên (index 0 đến 2) -> features[:3]\n",
    "        self.cnn = nn.Sequential(*list(effnet_full.features[:3]))\n",
    "        print(\"Đã xây dựng CNN bị cắt bớt SỚM HƠN từ EfficientNet-B0 pre-trained.\")\n",
    "\n",
    "        # Lấy số kênh output từ CNN đã tính toán trước đó\n",
    "        if 'CNN_OUTPUT_CHANNELS' not in globals():\n",
    "             raise NameError(\"Biến toàn cục CNN_OUTPUT_CHANNELS chưa được tính toán!\")\n",
    "        self.cnn_output_channels = CNN_OUTPUT_CHANNELS # Sẽ là output của block 2 (40)\n",
    "\n",
    "        # --- Adaptive Pooling để đảm bảo H_out = 1 ---\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        # --- Phần RNN ---\n",
    "        print(f\"Input size cho LSTM sẽ là: {self.cnn_output_channels}\") # Kênh ít hơn\n",
    "        self.rnn = nn.LSTM(self.cnn_output_channels, rnn_hidden_size, rnn_num_layers,\n",
    "                           bidirectional=True, batch_first=False, dropout=dropout if rnn_num_layers > 1 else 0)\n",
    "\n",
    "        self.dropout_rnn_out = nn.Dropout(dropout)\n",
    "\n",
    "        # --- Phần Fully Connected ---\n",
    "        self.fc = nn.Linear(rnn_hidden_size * 2, num_classes)\n",
    "\n",
    "        print(\"--- Khởi tạo CRNN với EfficientNet-B0 BỊ CẮT BỚT SỚM HƠN ---\")\n",
    "        print(f\"CNN Backbone: Truncated EfficientNet-B0 Features (ví dụ: [:3])\") # Cập nhật mô tả\n",
    "        print(f\"CNN Output Channels (to RNN): {self.cnn_output_channels}\")\n",
    "        print(f\"CNN Output Width (SeqLen for RNN - đã tính): {CNN_OUTPUT_WIDTH}\") # In ra để kiểm tra\n",
    "        # ... (Các thông số RNN/Dropout giữ nguyên) ...\n",
    "        print(f\"RNN Hidden Size: {rnn_hidden_size}\")\n",
    "        print(f\"RNN Num Layers: {rnn_num_layers}\")\n",
    "        print(f\"Output Classes (incl. blank): {num_classes}\")\n",
    "        print(\"------------------------------------------------------\")\n",
    "\n",
    "        del effnet_full\n",
    "        gc.collect()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ... (Logic forward giữ nguyên như trước) ...\n",
    "        features = self.cnn(x)\n",
    "        features = self.adaptive_pool(features)\n",
    "        features = features.squeeze(2)\n",
    "        features = features.permute(2, 0, 1)\n",
    "        self.rnn.flatten_parameters()\n",
    "        rnn_output, _ = self.rnn(features)\n",
    "        rnn_output = self.dropout_rnn_out(rnn_output)\n",
    "        output = self.fc(rnn_output)\n",
    "        return output"
   ],
   "id": "94f1803d48c1ca23",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:32.116460Z",
     "start_time": "2025-04-09T04:06:32.086839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === THAY ĐỔI: Khởi tạo model CRNN mới, kiểm tra device và forward ===\n",
    "# Kiểm tra device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nSử dụng device: {device}\")\n",
    "\n",
    "# Khởi tạo model CRNN với backbone EfficientNet\n",
    "# Đảm bảo num_classes_htr đã được tính toán chính xác\n",
    "if 'num_classes_htr' not in globals():\n",
    "     raise NameError(\"Biến num_classes_htr chưa được định nghĩa!\")\n",
    "\n",
    "print(f\"Khởi tạo model CRNN (EfficientNet) với {num_classes_htr} lớp đầu ra.\")\n",
    "model_htr = CRNN(num_classes=num_classes_htr) # Các tham số RNN/dropout dùng default\n",
    "model_htr = model_htr.to(device)\n",
    "print(\"Khởi tạo mô hình CRNN (EfficientNet Backbone) thành công.\")\n",
    "\n"
   ],
   "id": "10cefce4ac86f0a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sử dụng device: cpu\n",
      "Khởi tạo model CRNN (EfficientNet) với 162 lớp đầu ra.\n",
      "Input size cho LSTM sẽ là: 32\n",
      "--- Khởi tạo CRNN với SimpleCNN ---\n",
      "CNN Backbone: SimpleCNN\n",
      "CNN Output Channels (to RNN): 32\n",
      "CNN Output Width (SeqLen for RNN): 32\n",
      "RNN Hidden Size: 256\n",
      "RNN Num Layers: 2\n",
      "Output Classes (incl. blank): 162\n",
      "------------------------------------------------------\n",
      "Khởi tạo mô hình CRNN (EfficientNet Backbone) thành công.\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:32.303925Z",
     "start_time": "2025-04-09T04:06:32.137464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === THAY ĐỔI: Loss, Optimizer, Checkpoint handling (Cập nhật tên checkpoint và architecture) ===\n",
    "\n",
    "# ... (Phần định nghĩa criterion, optimizer giữ nguyên) ...\n",
    "if 'char_to_int' not in globals() or 'BLANK_TOKEN' not in globals(): #... (Kiểm tra blank_idx) ...\n",
    "    raise NameError(\"char_to_int hoặc BLANK_TOKEN chưa được định nghĩa!\")\n",
    "blank_idx = char_to_int.get(BLANK_TOKEN)\n",
    "if blank_idx is None: raise ValueError(f\"'{BLANK_TOKEN}' không tìm thấy trong char_to_int!\")\n",
    "criterion_htr = nn.CTCLoss(blank=blank_idx, reduction='mean', zero_infinity=True)\n",
    "print(f\"\\nĐã khởi tạo CTCLoss với blank index = {blank_idx}\")\n",
    "\n",
    "INITIAL_LR = 5e-4\n",
    "optimizer_htr = optim.AdamW(model_htr.parameters(), lr=INITIAL_LR, weight_decay=1e-2)\n",
    "print(f\"Đã khởi tạo Optimizer AdamW với LR = {INITIAL_LR}\")\n",
    "\n",
    "\n",
    "# --- CẬP NHẬT CHECKPOINT PATH VÀ ARCHITECTURE NAME ---\n",
    "CHECKPOINT_DIR = r\"G:\\Compvision_Final\\pythonProject\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "# Đổi tên file để phân biệt với kiến trúc TRUNCATED SỚM\n",
    "CHECKPOINT_FILENAME = \"checkpoint_htr_line_effnet_trunc_early.pth\" # Tên file mới hơn\n",
    "CHECKPOINT_PATH_HTR = os.path.join(CHECKPOINT_DIR, CHECKPOINT_FILENAME)\n",
    "# Sử dụng EXPECTED_ARCHITECTURE đã cập nhật ở cell tính toán width\n",
    "print(f\"Kiến trúc dự kiến: {EXPECTED_ARCHITECTURE}\")\n",
    "print(f\"Đường dẫn checkpoint mới: {CHECKPOINT_PATH_HTR}\")\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Training parameters\n",
    "START_EPOCH_HTR = 0\n",
    "NUM_EPOCHS = 17 # Đặt số epoch bạn muốn\n",
    "train_losses = []\n",
    "val_losses = [] # Vẫn giữ nếu bạn muốn thêm lại validation sau này\n",
    "\n",
    "# --- Load Checkpoint nếu có VÀ KIẾN TRÚC PHÙ HỢP (DÙNG TÊN MỚI NHẤT) ---\n",
    "if os.path.exists(CHECKPOINT_PATH_HTR):\n",
    "    print(f\"\\nTìm thấy checkpoint tại: {CHECKPOINT_PATH_HTR}\")\n",
    "    try:\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH_HTR, map_location=device)\n",
    "        print(\"Tải file checkpoint thành công.\")\n",
    "        # ... (Phần kiểm tra architecture và load state dict giữ nguyên logic như cell trước) ...\n",
    "        loaded_architecture = checkpoint.get('architecture')\n",
    "        if loaded_architecture != EXPECTED_ARCHITECTURE:\n",
    "             print(f\"\\nCẢNH BÁO: Checkpoint '{loaded_architecture}' != Hiện tại '{EXPECTED_ARCHITECTURE}'. Bắt đầu lại.\")\n",
    "             START_EPOCH_HTR = 0\n",
    "        else:\n",
    "             print(f\"Kiến trúc checkpoint khớp. Đang tải trạng thái...\")\n",
    "             # ... (load state dicts, epoch, losses, kiểm tra CNN width) ...\n",
    "             try:\n",
    "                 model_htr.load_state_dict(checkpoint['model_state_dict'])\n",
    "                 optimizer_htr.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                 START_EPOCH_HTR = checkpoint.get('epoch', -1) + 1\n",
    "                 train_losses = checkpoint.get('train_losses', [])\n",
    "                 loaded_cnn_output_width = checkpoint.get('cnn_output_width', -1)\n",
    "                 if loaded_cnn_output_width != -1 and loaded_cnn_output_width != CNN_OUTPUT_WIDTH:\n",
    "                      print(f\"\\nCẢNH BÁO: Width checkpoint ({loaded_cnn_output_width}) != hiện tại ({CNN_OUTPUT_WIDTH})!\")\n",
    "                 print(f\"\\n===> Đã tải checkpoint. Tiếp tục huấn luyện từ epoch {START_EPOCH_HTR}\")\n",
    "             except (KeyError, RuntimeError) as e_load:\n",
    "                 print(f\"\\nLỗi khi tải state_dict: {e_load}. Bắt đầu lại.\")\n",
    "                 START_EPOCH_HTR = 0; train_losses = []\n",
    "                 # Khởi tạo lại nếu cần\n",
    "                 if 'model_htr' not in locals(): model_htr = CRNN(num_classes=num_classes_htr).to(device)\n",
    "                 if 'optimizer_htr' not in locals(): optimizer_htr = optim.AdamW(model_htr.parameters(), lr=INITIAL_LR, weight_decay=5e-4)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nLỗi không xác định khi tải checkpoint: {e}. Bắt đầu lại.\")\n",
    "        START_EPOCH_HTR = 0; train_losses = []\n",
    "        # Khởi tạo lại nếu cần\n",
    "        if 'model_htr' not in locals(): model_htr = CRNN(num_classes=num_classes_htr).to(device)\n",
    "        if 'optimizer_htr' not in locals(): optimizer_htr = optim.AdamW(model_htr.parameters(), lr=INITIAL_LR, weight_decay=5e-4)\n",
    "else:\n",
    "    print(f\"\\nKhông tìm thấy checkpoint tại {CHECKPOINT_PATH_HTR}. Bắt đầu huấn luyện từ đầu (epoch 1).\")\n",
    "\n",
    "# Dọn dẹp\n",
    "if 'checkpoint' in locals(): del checkpoint\n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ],
   "id": "883e17220bda455c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Đã khởi tạo CTCLoss với blank index = 0\n",
      "Đã khởi tạo Optimizer AdamW với LR = 0.0005\n",
      "Kiến trúc dự kiến: EfficientNetB0_CRNN\n",
      "Đường dẫn checkpoint mới: G:\\Compvision_Final\\pythonProject\\checkpoint_htr_line_crnn_trunc_early.pth\n",
      "\n",
      "Không tìm thấy checkpoint tại G:\\Compvision_Final\\pythonProject\\checkpoint_htr_line_crnn_trunc_early.pth. Bắt đầu huấn luyện từ đầu (epoch 1).\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:32.335111Z",
     "start_time": "2025-04-09T04:06:32.320112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === KHÔNG THAY ĐỔI (Hàm giải mã Greedy) ===\n",
    "def decode_batch(outputs, int_to_char_map, blank_token_idx):\n",
    "    \"\"\"\n",
    "    Giải mã output của model (sau FC layer) thành chuỗi text sử dụng Greedy search.\n",
    "    Args:\n",
    "        outputs (torch.Tensor): Tensor output từ model. Shape: (SeqLen, Batch, NumClasses)\n",
    "        int_to_char_map (dict): Map từ index sang ký tự.\n",
    "        blank_token_idx (int): Index của blank token.\n",
    "    Returns:\n",
    "        list[str]: Danh sách các chuỗi text đã giải mã cho batch.\n",
    "    \"\"\"\n",
    "    # Lấy argmax theo chiều class (dim=2) -> Shape: (SeqLen, Batch)\n",
    "    # Chuyển sang CPU để xử lý numpy/list dễ hơn\n",
    "    preds = torch.argmax(outputs.detach().cpu(), dim=2)\n",
    "    # Chuyển vị thành (Batch, SeqLen)\n",
    "    preds = preds.permute(1, 0).numpy()\n",
    "\n",
    "    decoded_texts = []\n",
    "    for pred_seq in preds:\n",
    "        text = []\n",
    "        last_char_idx = -1 # Dùng index thay vì ký tự để so sánh\n",
    "        for idx in pred_seq:\n",
    "            # Bỏ qua blank token và các ký tự lặp lại liên tiếp\n",
    "            if idx != blank_token_idx and idx != last_char_idx:\n",
    "                char = int_to_char_map.get(idx)\n",
    "                if char: # Chỉ thêm nếu ký tự tồn tại trong map\n",
    "                    text.append(char)\n",
    "            last_char_idx = idx # Luôn cập nhật last_char_idx\n",
    "        decoded_texts.append(\"\".join(text))\n",
    "    return decoded_texts"
   ],
   "id": "e2d775cc61feb45e",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:32.380624Z",
     "start_time": "2025-04-09T04:06:32.367115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# scheduler = StepLR(optimizer_htr, step_size=3, gamma=0.7)"
   ],
   "id": "c404bbe1cd0ea333",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-09T04:06:32.434627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === THAY ĐỔI: Vòng lặp Huấn luyện (BỎ HOÀN TOÀN VALIDATION) ===\n",
    "\n",
    "# Kiểm tra DataLoader trước khi bắt đầu\n",
    "if not train_loader_htr:\n",
    "     print(\"\\nLỗi: train_loader_htr không hợp lệ. Không thể bắt đầu huấn luyện.\")\n",
    "     exit() # Nên dừng ở đây nếu không có dữ liệu huấn luyện\n",
    "\n",
    "print(f\"\\nBắt đầu huấn luyện từ epoch {START_EPOCH_HTR + 1} đến {NUM_EPOCHS} (KHÔNG Validation)\")\n",
    "\n",
    "# Reset lại list loss nếu bắt đầu từ đầu\n",
    "if START_EPOCH_HTR == 0:\n",
    "    train_losses = []\n",
    "    # val_losses = [] # Không cần val_losses nữa\n",
    "\n",
    "\n",
    "for epoch in range(START_EPOCH_HTR, NUM_EPOCHS):\n",
    "    epoch_num = epoch + 1\n",
    "    print(f\"\\n===== Epoch {epoch_num}/{NUM_EPOCHS} =====\")\n",
    "\n",
    "    # --- Training Phase ---\n",
    "    model_htr.train() # Đặt model ở chế độ training\n",
    "    running_loss = 0.0\n",
    "    num_train_samples_processed = 0\n",
    "    train_batches_processed = 0\n",
    "\n",
    "    train_progress_bar = tqdm(train_loader_htr, desc=f\"Epoch {epoch_num} [Train]\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(train_progress_bar):\n",
    "        # Kiểm tra xem collate_fn có trả về None không\n",
    "        if batch_data is None or batch_data[0] is None:\n",
    "            continue # Bỏ qua batch không hợp lệ\n",
    "\n",
    "        images, padded_labels, input_lengths, label_lengths = batch_data\n",
    "        current_batch_size = images.size(0)\n",
    "\n",
    "        # Chuyển dữ liệu lên device\n",
    "        images = images.to(device)\n",
    "        padded_labels = padded_labels.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "\n",
    "        # --- Forward pass ---\n",
    "        optimizer_htr.zero_grad()\n",
    "        outputs = model_htr(images) # output shape: (SeqLen, Batch, NumClasses)\n",
    "\n",
    "        # --- Calculate CTC Loss ---\n",
    "        log_probs = nn.functional.log_softmax(outputs, dim=2)\n",
    "\n",
    "        # Điều chỉnh input lengths nếu cần (dù không nên xảy ra nếu CNN_OUTPUT_WIDTH đúng)\n",
    "        valid_input_lengths = torch.clamp(input_lengths, max=log_probs.shape[0])\n",
    "        # Đảm bảo label_lengths > 0\n",
    "        valid_label_lengths = torch.clamp(label_lengths, min=1)\n",
    "\n",
    "        # Tính loss\n",
    "        try:\n",
    "            loss = criterion_htr(log_probs, padded_labels, valid_input_lengths, valid_label_lengths)\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"\\nWarning: NaN/Inf loss in train batch {batch_idx}. Skipping backward pass.\")\n",
    "                continue\n",
    "\n",
    "            # --- Backward pass and Optimization ---\n",
    "            loss.backward()\n",
    "            optimizer_htr.step()\n",
    "\n",
    "            # Tích lũy loss\n",
    "            running_loss += loss.item() * current_batch_size\n",
    "            num_train_samples_processed += current_batch_size\n",
    "            train_batches_processed += 1\n",
    "\n",
    "            # Cập nhật thanh progress bar\n",
    "            train_progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{optimizer_htr.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "        except RuntimeError as rte:\n",
    "             if \"targets must be shorter than the T dimension\" in str(rte):\n",
    "                  # print(f\"\\nLỗi CTC trong train batch {batch_idx}: Target length > Input length. Skipping.\") # Có thể bật để debug\n",
    "                  # print(f\"  Input lengths (max {log_probs.shape[0]}): {valid_input_lengths.tolist()}\")\n",
    "                  # print(f\"  Target lengths: {valid_label_lengths.tolist()}\")\n",
    "                  continue\n",
    "             else:\n",
    "                  print(f\"\\nLỗi RuntimeError trong training step (batch {batch_idx}): {rte}\")\n",
    "                  traceback.print_exc()\n",
    "                  continue\n",
    "        except Exception as e:\n",
    "            print(f\"\\nLỗi không xác định trong training step (batch {batch_idx}): {e}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    # Tính loss trung bình cho epoch training\n",
    "    if num_train_samples_processed > 0:\n",
    "        avg_train_loss = running_loss / num_train_samples_processed\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch_num} [Train] Average Loss: {avg_train_loss:.4f} ({train_batches_processed} batches)\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch_num} [Train] No batches processed successfully.\")\n",
    "        train_losses.append(float('inf')) # Ghi nhận lỗi nếu không có batch nào thành công\n",
    "\n",
    "\n",
    "    # --- KHÔNG CÓ VALIDATION PHASE ---\n",
    "\n",
    "\n",
    "    # # --- Adjust Learning Rate (Optional - Không dựa vào val_loss) ---\n",
    "    # # Ví dụ: Giảm LR sau một số epoch cố định\n",
    "    # if 'scheduler' in locals() and isinstance(scheduler, optim.lr_scheduler.StepLR):\n",
    "    #      scheduler.step()\n",
    "    #      print(f\"Current LR: {optimizer_htr.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "\n",
    "    # --- Lưu Checkpoint SAU MỖI EPOCH ---\n",
    "    # Chỉ lưu checkpoint cuối cùng của epoch đó\n",
    "    checkpoint_data = {\n",
    "        'epoch': epoch,\n",
    "        'architecture': EXPECTED_ARCHITECTURE,\n",
    "        'model_state_dict': model_htr.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_htr.state_dict(),\n",
    "        'char_to_int': char_to_int,\n",
    "        'train_losses': train_losses,\n",
    "        # 'val_losses': [], # Không lưu val_losses\n",
    "        # 'best_val_loss': float('inf'), # Không lưu best_val_loss\n",
    "        'target_img_width': TARGET_IMG_WIDTH,\n",
    "        'target_img_height': TARGET_IMG_HEIGHT,\n",
    "        'cnn_output_width': CNN_OUTPUT_WIDTH\n",
    "    }\n",
    "    try:\n",
    "        torch.save(checkpoint_data, CHECKPOINT_PATH_HTR)\n",
    "        # Bỏ comment dòng dưới nếu muốn thấy log lưu mỗi epoch\n",
    "        # print(f\"Checkpoint saved after epoch {epoch_num} to {CHECKPOINT_PATH_HTR}\")\n",
    "    except Exception as e_save:\n",
    "         print(f\"Lỗi khi lưu checkpoint tại epoch {epoch_num}: {e_save}\")\n",
    "\n",
    "\n",
    "# --- Kết thúc vòng lặp huấn luyện ---\n",
    "print(\"\\n===== Hoàn thành huấn luyện (Không có validation)! =====\")\n",
    "\n",
    "# Vẽ đồ thị loss (chỉ có train loss)\n",
    "if train_losses:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Lưu đồ thị nếu muốn\n",
    "    loss_plot_path = os.path.join(CHECKPOINT_DIR, f\"loss_plot_{EXPECTED_ARCHITECTURE}_train_only.png\")\n",
    "    try:\n",
    "        plt.savefig(loss_plot_path)\n",
    "        print(f\"Đã lưu đồ thị loss vào: {loss_plot_path}\")\n",
    "    except Exception as e_plot:\n",
    "        print(f\"Lỗi khi lưu đồ thị loss: {e_plot}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Không có dữ liệu train loss để vẽ đồ thị.\")"
   ],
   "id": "4509981b916d8df2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bắt đầu huấn luyện từ epoch 1 đến 3 (KHÔNG Validation)\n",
      "\n",
      "===== Epoch 1/3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/183 [00:00<?, ?batch/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   1%|          | 1/183 [00:00<00:28,  6.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([79, 81, 80, 51, 79, 82, 73, 82, 28, 70, 59, 89, 78, 73, 45, 69, 66, 95,\n",
      "        67, 55, 65, 78, 68, 81, 71, 47, 59, 23, 48, 71, 58, 67])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   1%|          | 2/183 [00:00<00:23,  7.76batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([62, 52, 75, 60, 69, 63, 30, 54, 85, 72, 45, 69, 79, 55, 67, 75, 79, 67,\n",
      "        70, 74, 81, 21, 68, 90, 64, 70, 67, 90, 69, 74, 78, 78])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   2%|▏         | 3/183 [00:00<00:23,  7.80batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([83, 33, 69, 15, 63, 74, 83, 96, 92, 75, 83, 25, 64, 75, 62, 96, 80, 82,\n",
      "        69, 76, 52, 53, 26, 82, 67, 64, 57, 88, 75, 72, 76, 74])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([65, 82, 59, 82, 68, 69, 85, 70, 75, 25, 98, 89, 76, 80, 72, 54, 52, 66,\n",
      "        81, 77, 85, 73, 21, 88, 67, 76, 65, 54, 67, 64, 19, 64])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   3%|▎         | 5/183 [00:00<00:21,  8.34batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([71, 72, 62, 84, 80, 35, 62, 74, 89, 66, 75, 16, 46, 70, 74, 13, 70, 83,\n",
      "        90, 57, 30, 54, 70, 43, 65, 68, 80, 81, 72, 68, 20, 82])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   3%|▎         | 6/183 [00:00<00:21,  8.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([83, 95, 85, 99, 58, 61, 44, 69, 82, 95, 73, 58, 79, 71, 57, 87, 35, 73,\n",
      "        76, 72, 81, 56, 74, 21,  8, 71, 63, 68, 67, 65, 66, 69])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   4%|▍         | 7/183 [00:00<00:21,  8.22batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([72, 42, 89, 88, 63, 81, 28, 69, 24, 68, 64, 76, 71, 28, 21, 71, 70, 54,\n",
      "        33, 68, 57, 73, 97, 70, 24, 29, 32, 76, 56, 71, 63, 78])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   4%|▍         | 8/183 [00:00<00:21,  8.17batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([67, 28, 65, 37, 81, 74, 66, 66, 88, 70, 72, 88, 61, 60, 64, 51, 78, 68,\n",
      "        81, 93, 95, 72, 35, 76, 74, 61, 15, 61, 53, 78, 60, 73])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   5%|▍         | 9/183 [00:01<00:21,  7.92batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 80,  71,  72,  71,  76,  81,  70,  76,  84,  65,  69,  85,  37,  16,\n",
      "        109,  74,  78,  58,  73,  77,  58,  71,  76,  72,  76,  83,  74,  78,\n",
      "         93,  71,   8,  65])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   5%|▌         | 10/183 [00:01<00:22,  7.70batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 75,  53,  42,  68,  66,  63,  91,  82,  80,  79,  87,  63,  21,  72,\n",
      "         73,  90, 104,  69,  85,  91,  35,  67,  73,  76,  73,  63,  59,  72,\n",
      "         53,  73,  57,  81])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   6%|▌         | 11/183 [00:01<00:21,  7.89batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([74, 55, 80, 61, 73, 68, 74, 72, 67, 73, 72, 97, 96, 27, 63, 72, 76, 12,\n",
      "        35, 74, 25, 72, 61, 70, 62, 79, 73, 68, 65, 71, 58, 71])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   7%|▋         | 12/183 [00:01<00:21,  7.93batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 74,  83, 109,  97,  89,  56,  57,  77,  75,  88,  46,  80,  69,  83,\n",
      "         81,  87,  75,  77,  16,  57,  67,  64,  95,  59,  10,  81,  39,  65,\n",
      "         74,  72,  99,  59])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   7%|▋         | 13/183 [00:01<00:21,  7.77batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([61, 77, 81, 35, 79, 62, 67, 67, 84, 83, 66, 77, 21, 21, 65, 73, 76, 67,\n",
      "        73, 20, 63, 77, 87, 74, 64, 70, 76, 65, 66, 84, 52, 70])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   8%|▊         | 14/183 [00:01<00:21,  8.01batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 64,  62,  62,  35,  19,  85,  30,  72,  64,  68, 102,  68,  47,  12,\n",
      "         81,  67,  67,  94,  83,  77,   4,  53,  78,  56,  93,  70,  64,  52,\n",
      "         78,  63,  51,  84])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   8%|▊         | 15/183 [00:01<00:19,  8.48batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([74, 80, 84, 47, 64, 63, 97, 21, 63, 82, 71, 64, 63, 70, 77, 88, 80, 42,\n",
      "        71, 86, 81, 81, 79, 72, 85, 25, 73, 74, 64, 83, 77, 69])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   9%|▊         | 16/183 [00:02<00:21,  7.64batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([71, 55, 27, 29, 65, 59, 91, 70, 74, 79, 66, 83, 76, 81, 43, 97, 58, 90,\n",
      "        91, 61, 57, 92, 79, 80, 28, 66, 87, 64, 59, 64, 58, 58])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:   9%|▉         | 17/183 [00:02<00:20,  7.96batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([66, 72, 49, 83, 85, 65, 70, 44, 68, 69, 71, 79, 66, 78, 72, 18, 62, 62,\n",
      "        69, 78, 70, 76, 72, 90, 78, 82, 70, 68, 72, 72, 90, 83])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  10%|▉         | 18/183 [00:02<00:21,  7.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 68, 101,  75,  44,  88,  74,  27,  67,  40,  79,  61,  79,  86,  83,\n",
      "         59,  77,  83,  80,  60,  58,  63,  75,  69,  77,  14,  70,  91,  83,\n",
      "         75,  60,  65,  27])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  10%|█         | 19/183 [00:02<00:21,  7.67batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([59, 66, 67, 65, 71, 64, 60, 64, 71, 81, 65,  9, 61, 73, 76, 67, 77, 76,\n",
      "        67, 73, 76, 78, 63, 58, 74, 33, 12, 64, 83, 91, 71, 82])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  11%|█         | 20/183 [00:02<00:21,  7.70batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([79, 66, 71, 55, 89, 72, 68, 76, 52, 72, 79, 55, 26, 19, 59, 64, 84, 56,\n",
      "        80, 60, 90, 63,  6, 56, 99, 63, 74, 76, 65, 74, 87, 30])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  11%|█▏        | 21/183 [00:02<00:21,  7.66batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 73,  63,  59,  73,  68,  72,  88,  66,  13,  60, 104,  98,  74,  79,\n",
      "         82,  77,  65,  75,  80,  74,  84,  79,  82,  73,  64,  73,  92,  74,\n",
      "         73,  59,  85,  60])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 63,  61,  66,  72,  77,  82,  74,  66,  67,  75,  81,  67,  75,  66,\n",
      "         70,  82,  15,  81,  76,  78,  67,  68,  68,  74,  70,  57,  69,  82,\n",
      "         78, 100,  81,  64])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  13%|█▎        | 23/183 [00:02<00:18,  8.78batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([55, 74, 79, 74, 79,  6, 70, 76, 55, 68, 63, 67, 89, 71, 70, 70, 88, 86,\n",
      "        67, 51, 65, 31,  9, 67, 63, 54, 48, 75, 78, 85, 91, 94])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  13%|█▎        | 24/183 [00:02<00:18,  8.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([89, 51, 78, 54, 73, 91, 89, 69, 13, 82, 69, 63, 69, 73, 52, 81, 72, 50,\n",
      "        64, 93, 90, 60, 57, 71, 78, 99, 78, 55, 73, 72, 71, 65])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  14%|█▎        | 25/183 [00:03<00:19,  8.14batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([78, 81, 40, 95,  5, 59, 65, 89, 76, 49, 66,  3, 60, 58, 58, 43, 89, 65,\n",
      "        86, 52, 88, 90, 74, 83, 65, 78, 83, 93, 83, 90, 71, 81])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  14%|█▍        | 26/183 [00:03<00:18,  8.39batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 68,  69,  69,  33,  73,  74,  58,  77,  64,  61,  83, 125,  64,  79,\n",
      "         78,  61,  61,  53,  90,  90,  72,  79,  95,  69,  59,  73,  88,  49,\n",
      "         74,  72,  52,  86])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  15%|█▍        | 27/183 [00:03<00:19,  8.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([68, 74, 34, 75, 51, 97, 72, 67, 79, 71, 22, 19, 84, 55, 10, 72, 75, 54,\n",
      "        92, 77, 59, 78, 75, 76, 78, 64, 83, 95, 64, 57, 55, 88])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  15%|█▌        | 28/183 [00:03<00:19,  7.89batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([73, 90, 76, 78, 75, 25, 30, 77, 84, 70, 70, 86, 71, 55, 93, 80, 65, 65,\n",
      "        78, 62, 82, 76, 69, 61, 63, 82, 70, 51, 71, 73, 95, 44])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  16%|█▌        | 29/183 [00:03<00:19,  8.01batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([  5,  78,  15,  82,  83,  93,  41,  68,  73,  79,  73,  60,  61,  75,\n",
      "         84,  63,  66, 101,  78,  64,  89,  35,  51,  21,  75,  64,  70,  54,\n",
      "        104,  83,  67,  35])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  16%|█▋        | 30/183 [00:03<00:19,  7.74batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([79, 71, 87, 69, 58, 51, 77,  8, 66, 62, 67, 94, 72, 87, 66, 55, 15, 74,\n",
      "        86, 38, 71, 77, 83, 68, 74, 81, 61, 68, 67, 73, 79, 94])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  17%|█▋        | 31/183 [00:03<00:18,  8.28batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([85, 65, 80, 57, 64, 62, 68, 67, 72, 66, 29, 59, 84,  4, 79, 62, 73,  5,\n",
      "        75, 42, 75, 76, 80, 77, 39, 52, 64,  5, 80, 70, 66, 84])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([69, 69, 66, 47, 60, 68, 72, 66, 66, 66, 68,  6, 81, 58, 81, 66, 77, 65,\n",
      "        71, 68, 77, 63, 79, 38, 81, 23, 65, 63, 56, 75, 32, 55])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  18%|█▊        | 33/183 [00:04<00:16,  8.87batch/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  19%|█▊        | 34/183 [00:04<00:17,  8.75batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 83,  65,  77, 105,  63,  58,  62,  68,  71,  69,  74,  76,  65,  35,\n",
      "         71,  74,  86,  63,  81,  91,  74,  85,  20,  67,  78,  63,  88,  33,\n",
      "         78,  62,  60,  64])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([77, 60, 57, 37, 61, 62, 78, 67, 71, 77, 62, 77, 85, 69, 59, 85, 75, 33,\n",
      "        79, 61, 66, 35, 62, 74, 76, 78, 66, 80, 78, 69, 76, 72])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  20%|█▉        | 36/183 [00:04<00:16,  9.06batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([75, 63, 68, 61, 74, 80, 59, 62, 65, 75, 83, 25, 73, 88, 71, 66, 77, 65,\n",
      "        69, 79, 68, 87, 92, 89, 68, 53, 90, 55, 23, 68, 61, 58])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([58, 73, 12, 84, 70, 73, 67, 87, 78, 67, 79, 27, 70, 61, 81, 70, 91, 72,\n",
      "        76, 82, 68, 74, 53, 81, 70, 68, 20, 69, 78, 70, 58, 63])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  20%|██        | 37/183 [00:04<00:16,  9.11batch/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  21%|██        | 38/183 [00:04<00:16,  8.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([26, 75, 75,  4, 54, 51, 84, 64, 74, 74, 64, 55, 58, 82, 99, 63,  4, 75,\n",
      "        83, 69, 78, 68, 74, 69, 87, 76, 65, 34, 59,  5, 83, 84])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([75, 16, 62, 82, 78, 70, 65, 57, 16, 71, 77, 64, 83, 63, 78, 36, 86, 18,\n",
      "        77, 60, 66, 63, 69, 74, 34, 75, 74, 71, 21, 83, 76, 77])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  21%|██▏       | 39/183 [00:04<00:17,  8.28batch/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  22%|██▏       | 40/183 [00:04<00:17,  8.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([68, 68, 39, 66, 72, 60, 83, 45, 75, 60, 88, 68, 88, 69, 62, 67, 97, 75,\n",
      "        82, 68, 63, 49, 69, 49, 81, 71, 78, 75, 19, 74, 70, 65])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 68,  58,  15,  88,  37,  78,  74,  74,  71,  71,  79,  74,  52,  74,\n",
      "         30,  67,  63,  71,  85,  79,  71,  81, 103,  78,  58,  80,  61,  15,\n",
      "         78,  27,  67,  62])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  22%|██▏       | 41/183 [00:05<00:16,  8.43batch/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  23%|██▎       | 42/183 [00:05<00:17,  8.25batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([73, 79, 68, 72, 12, 84, 72, 68, 44, 76, 68, 74, 52, 65, 66, 70, 60, 40,\n",
      "        78, 69, 72, 67, 56, 57, 83,  6, 80, 25, 75, 71, 75, 19])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([58, 54, 65,  5, 85, 77, 59, 68, 48, 73, 18, 85, 27, 13, 60, 10, 21, 70,\n",
      "        72, 29, 69, 82, 66, 77, 76, 75, 77, 58, 66, 68, 82, 65])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  23%|██▎       | 43/183 [00:05<00:17,  8.12batch/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  24%|██▍       | 44/183 [00:05<00:17,  7.73batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([63, 81, 82, 61, 83, 83, 83, 55, 11, 67, 75, 78, 65, 80, 58, 86, 59, 69,\n",
      "        89, 73, 72, 69, 68, 60, 75, 67, 69, 74, 71, 97, 67, 63])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 75,  78,  63,  65,  78,  80,  74, 104,  83,  65, 110,  73,  47,  75,\n",
      "         73,  12,  91,  68,  90,  70,  78,  72,  67,  83,  51,  85,  80,  20,\n",
      "         88,  66,  16,  42])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  25%|██▍       | 45/183 [00:05<00:18,  7.38batch/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  25%|██▌       | 46/183 [00:05<00:17,  7.91batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([65, 78, 92, 90, 83, 65, 79, 58, 74, 68, 74, 68, 83, 77, 75, 80, 75, 70,\n",
      "        39, 90, 67, 59, 88, 70, 18, 68, 79, 86, 73, 69, 82, 30])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([73, 68, 53, 76, 78, 78, 66, 71, 68, 64, 61, 63, 66, 52, 87, 39, 63, 66,\n",
      "        87, 71, 80, 66, 64, 63, 15, 78, 77, 68, 62, 80, 86, 56])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  26%|██▌       | 47/183 [00:05<00:17,  7.97batch/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  26%|██▌       | 48/183 [00:05<00:16,  8.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([70, 85, 72, 66, 79, 57, 72, 85, 85, 69, 79, 53, 48, 13, 73, 70, 76, 73,\n",
      "        64, 61, 84, 65, 68, 63, 48, 88, 79, 75, 70, 65, 21, 75])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 66,  66,  86,  59,  71,  11,  85,  82,  90,  77,  61,  63,  76,  73,\n",
      "         67,  86,  73,  40,  73,  88,  73,  68,  70,  69,  77,  61,  76, 106,\n",
      "         73,   5,  64,  84])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  27%|██▋       | 49/183 [00:06<00:16,  8.11batch/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_67524\\3383103312.py\", line 47, in collate_fn_htr\n",
      "    raise NameError(\"CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\")\n",
      "NameError: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "Epoch 1 [Train]:  27%|██▋       | 50/183 [00:06<00:16,  8.06batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([33, 60, 11, 72, 63, 77, 81, 36, 64, 72, 89, 73, 48, 71, 89, 72, 53, 75,\n",
      "        76, 57, 27, 84, 72, 82, 65, 42, 84, 31, 76, 80, 73, 60])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n",
      "collate_fn_htr: Stacking images...\n",
      "collate_fn_htr: Images shape: torch.Size([32, 1, 32, 128])\n",
      "collate_fn_htr: Calculating label lengths...\n",
      "collate_fn_htr: Initial label_lengths: tensor([ 75,  69,  55,  64,  54,  78,  71,  66,  77,  80,  80,  82,  87,  74,\n",
      "         63,  93,  84,  75,  75,  92,  77,  64,  86,  81,  68,  86,  81, 109,\n",
      "         59,  98,  85,  79])\n",
      "collate_fn_htr: Getting blank token index...\n",
      "collate_fn_htr: Blank token index: 0\n",
      "collate_fn_htr: Checking for labels exceeding CNN_OUTPUT_WIDTH...\n",
      "CRITICAL ERROR in collate_fn_htr: CNN_OUTPUT_WIDTH not calculated yet. Run CNN output width calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[86], line 28\u001B[0m\n\u001B[0;32m     24\u001B[0m train_batches_processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     26\u001B[0m train_progress_bar \u001B[38;5;241m=\u001B[39m tqdm(train_loader_htr, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m [Train]\u001B[39m\u001B[38;5;124m\"\u001B[39m, unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, batch_data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_progress_bar):\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;66;03m# Kiểm tra xem collate_fn có trả về None không\u001B[39;00m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m batch_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m batch_data[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     31\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m \u001B[38;5;66;03m# Bỏ qua batch không hợp lệ\u001B[39;00m\n",
      "File \u001B[1;32mD:\\coding\\CompvisionProject\\.venv\\lib\\site-packages\\tqdm\\std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1182\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1183\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1184\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\coding\\CompvisionProject\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    705\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    707\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 708\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    709\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    710\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    711\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    712\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    713\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    714\u001B[0m ):\n",
      "File \u001B[1;32mD:\\coding\\CompvisionProject\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    762\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    763\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 764\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    765\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    766\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\coding\\CompvisionProject\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\coding\\CompvisionProject\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[76], line 44\u001B[0m, in \u001B[0;36mCustomDatasetHTR.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m# Áp dụng transform (bao gồm ToTensor, repeat channels, Normalize)\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform:\n\u001B[1;32m---> 44\u001B[0m     image_tensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m      \u001B[38;5;66;03m# Nếu không có transform, cần tự chuyển sang Tensor và xử lý kênh\u001B[39;00m\n\u001B[0;32m     47\u001B[0m      \u001B[38;5;66;03m# Nên luôn có transform cho model PyTorch\u001B[39;00m\n\u001B[0;32m     48\u001B[0m      image_tensor \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mToTensor()(image) \u001B[38;5;66;03m# Chỉ chuyển thành Tensor (1, H, W)\u001B[39;00m\n",
      "File \u001B[1;32mD:\\coding\\CompvisionProject\\.venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32mD:\\coding\\CompvisionProject\\.venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\coding\\CompvisionProject\\.venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;66;03m# handle PIL Image\u001B[39;00m\n\u001B[0;32m    167\u001B[0m mode_to_nptype \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI\u001B[39m\u001B[38;5;124m\"\u001B[39m: np\u001B[38;5;241m.\u001B[39mint32, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI;16\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mbyteorder \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlittle\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI;16B\u001B[39m\u001B[38;5;124m\"\u001B[39m: np\u001B[38;5;241m.\u001B[39mint16, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mF\u001B[39m\u001B[38;5;124m\"\u001B[39m: np\u001B[38;5;241m.\u001B[39mfloat32}\n\u001B[1;32m--> 168\u001B[0m img \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode_to_nptype\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muint8\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pic\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    171\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m255\u001B[39m \u001B[38;5;241m*\u001B[39m img\n",
      "File \u001B[1;32mD:\\coding\\CompvisionProject\\.venv\\lib\\site-packages\\PIL\\Image.py:735\u001B[0m, in \u001B[0;36mImage.__array_interface__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    733\u001B[0m     new[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtobytes(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    734\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 735\u001B[0m     new[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtobytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    736\u001B[0m new[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m], new[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtypestr\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m _conv_type_shape(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    737\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m new\n",
      "File \u001B[1;32mD:\\coding\\CompvisionProject\\.venv\\lib\\site-packages\\PIL\\Image.py:794\u001B[0m, in \u001B[0;36mImage.tobytes\u001B[1;34m(self, encoder_name, *args)\u001B[0m\n\u001B[0;32m    791\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m encoder_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m encoder_args \u001B[38;5;241m==\u001B[39m ():\n\u001B[0;32m    792\u001B[0m     encoder_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode\n\u001B[1;32m--> 794\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheight \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    797\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mD:\\coding\\CompvisionProject\\.venv\\lib\\site-packages\\PIL\\ImageFile.py:385\u001B[0m, in \u001B[0;36mImageFile.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    382\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[0;32m    384\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[1;32m--> 385\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    387\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:38.641547300Z",
     "start_time": "2025-04-08T00:45:18.935330Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"DEBUG: Batch {batch_idx}, LogProbs SeqLen: {log_probs.shape[0]}, InputLengths Max: {valid_input_lengths.max().item()}, LabelLengths Max: {valid_label_lengths.max().item()}\")",
   "id": "13cc401cbd91c24a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Batch 182, LogProbs SeqLen: 96, InputLengths Max: 96, LabelLengths Max: 89\n"
     ]
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:38.642548500Z",
     "start_time": "2025-04-07T23:50:27.887602Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install jiwer",
   "id": "835777769b7aafe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in d:\\coding\\compvisionproject\\.venv\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in d:\\coding\\compvisionproject\\.venv\\lib\\site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in d:\\coding\\compvisionproject\\.venv\\lib\\site-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: colorama in d:\\coding\\compvisionproject\\.venv\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:38.642548500Z",
     "start_time": "2025-04-08T00:45:26.396218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === KHÔNG THAY ĐỔI (Import jiwer để đánh giá) ===\n",
    "try:\n",
    "    import jiwer\n",
    "    print(\"jiwer library imported successfully.\")\n",
    "    # Định nghĩa transformation cơ bản cho jiwer (thường là lowercase và bỏ dấu câu)\n",
    "    jiwer_transform = jiwer.Compose([\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemoveMultipleSpaces(),\n",
    "        # jiwer.RemovePunctuation(),\n",
    "        jiwer.Strip()\n",
    "    ])\n",
    "    print(\"Defined basic jiwer transformation.\")\n",
    "except ImportError:\n",
    "    print(\"Lỗi: Thư viện jiwer chưa được cài đặt.\")\n",
    "    print(\"Vui lòng cài đặt bằng lệnh: pip install jiwer\")\n",
    "    jiwer = None # Đặt là None để kiểm tra trước khi sử dụng\n",
    "    jiwer_transform = None"
   ],
   "id": "67e22dc3316458f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jiwer library imported successfully.\n",
      "Defined basic jiwer transformation.\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:38.643547200Z",
     "start_time": "2025-04-08T00:45:28.271920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === KHÔNG THAY ĐỔI (Hàm giải mã nhãn gốc từ dạng padded) ===\n",
    "def decode_padded_labels(padded_labels, label_lengths, int_to_char_map):\n",
    "    \"\"\"\n",
    "    Chuyển đổi batch label đã được pad và tensor độ dài tương ứng\n",
    "    thành một list các chuỗi text gốc.\n",
    "    Args:\n",
    "        padded_labels (torch.Tensor): Shape: (Batch, MaxLength). Nên ở trên CPU.\n",
    "        label_lengths (torch.Tensor): Shape: (Batch,). Nên ở trên CPU.\n",
    "        int_to_char_map (dict): Map từ index sang ký tự.\n",
    "    Returns:\n",
    "        list[str]: List các chuỗi text gốc đã được giải mã.\n",
    "    \"\"\"\n",
    "    ground_truth_texts = []\n",
    "    padded_labels_cpu = padded_labels.cpu()\n",
    "    label_lengths_cpu = label_lengths.cpu()\n",
    "    for i in range(padded_labels_cpu.size(0)):\n",
    "        actual_len = label_lengths_cpu[i].item()\n",
    "        if actual_len > 0:\n",
    "            label_indices = padded_labels_cpu[i][:actual_len].tolist()\n",
    "            try:\n",
    "                text = \"\".join([int_to_char_map.get(idx, '') for idx in label_indices])\n",
    "                ground_truth_texts.append(text)\n",
    "            except Exception as e:\n",
    "                print(f\"Error decoding GT indices sample {i}: {e}\")\n",
    "                ground_truth_texts.append(\"[DECODING ERROR GT]\")\n",
    "        else:\n",
    "             ground_truth_texts.append(\"\")\n",
    "    return ground_truth_texts"
   ],
   "id": "d6cc2aab95d72a43",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:38.643547200Z",
     "start_time": "2025-04-08T00:45:30.611756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === VIẾT LẠI: Đánh giá cuối cùng trên tập Test ===\n",
    "\n",
    "print(\"\\n===== Bắt đầu Đánh giá Cuối cùng trên Tập Test =====\")\n",
    "\n",
    "# --- Khởi tạo biến cho đánh giá ---\n",
    "model_to_evaluate = None\n",
    "checkpoint_loaded_successfully = False\n",
    "char_to_int_eval = None\n",
    "int_to_char_eval = None\n",
    "blank_idx_eval = None\n",
    "num_classes_eval = None\n",
    "\n",
    "# --- Kiểm tra điều kiện tiên quyết ---\n",
    "if jiwer is None:\n",
    "    print(\"Lỗi: Thư viện jiwer không khả dụng.\")\n",
    "elif 'test_loader_htr' not in globals() or test_loader_htr is None:\n",
    "    print(\"Lỗi: test_loader_htr không được định nghĩa hoặc rỗng.\")\n",
    "elif 'CHECKPOINT_PATH_HTR' not in globals() or not CHECKPOINT_PATH_HTR:\n",
    "    print(\"Lỗi: Đường dẫn checkpoint CHECKPOINT_PATH_HTR chưa được định nghĩa.\")\n",
    "elif 'EXPECTED_ARCHITECTURE' not in globals() or not EXPECTED_ARCHITECTURE:\n",
    "    print(\"Lỗi: Tên kiến trúc EXPECTED_ARCHITECTURE chưa được định nghĩa.\")\n",
    "elif 'CRNN' not in globals() or not callable(CRNN):\n",
    "     print(\"Lỗi: Class CRNN chưa được định nghĩa.\")\n",
    "elif 'BLANK_TOKEN' not in globals():\n",
    "     print(\"Lỗi: Biến BLANK_TOKEN chưa được định nghĩa.\")\n",
    "else:\n",
    "    # --- Tải Checkpoint và Model ---\n",
    "    load_path = CHECKPOINT_PATH_HTR # Sử dụng path đã định nghĩa cho training\n",
    "    print(f\"Đường dẫn checkpoint để đánh giá: {load_path}\")\n",
    "\n",
    "    if os.path.exists(load_path):\n",
    "        try:\n",
    "            print(f\"Đang tải checkpoint từ: {load_path}\")\n",
    "            checkpoint = torch.load(load_path, map_location=device)\n",
    "            print(\"Tải file checkpoint thành công.\")\n",
    "\n",
    "            # 1. Kiểm tra kiến trúc\n",
    "            loaded_architecture = checkpoint.get('architecture')\n",
    "            if loaded_architecture != EXPECTED_ARCHITECTURE:\n",
    "                print(f\"\\nLỖI KIẾN TRÚC: Checkpoint là '{loaded_architecture}', nhưng đang cần '{EXPECTED_ARCHITECTURE}'.\")\n",
    "            else:\n",
    "                print(f\"-> Kiến trúc checkpoint khớp: '{loaded_architecture}'\")\n",
    "\n",
    "                # 2. Load Character Map (Bắt buộc)\n",
    "                if 'char_to_int' in checkpoint:\n",
    "                    char_to_int_eval = checkpoint['char_to_int']\n",
    "                    int_to_char_eval = {v: k for k, v in char_to_int_eval.items()}\n",
    "                    num_classes_eval = len(char_to_int_eval)\n",
    "                    blank_idx_eval = char_to_int_eval.get(BLANK_TOKEN)\n",
    "                    if blank_idx_eval is None:\n",
    "                         print(f\"Lỗi: '{BLANK_TOKEN}' không tìm thấy trong char_to_int của checkpoint.\")\n",
    "                    else:\n",
    "                         print(f\"-> Load thành công char_map ({num_classes_eval} lớp, blank={blank_idx_eval}).\")\n",
    "\n",
    "                         # 3. Khởi tạo Model đúng cấu trúc\n",
    "                         print(\"--> Khởi tạo cấu trúc model CRNN...\")\n",
    "                         model_to_evaluate = CRNN(num_classes=num_classes_eval).to(device)\n",
    "                         # CRNN() ở đây phải là class đã được định nghĩa ở cell trước với cấu trúc CNN cắt bớt đúng\n",
    "\n",
    "                         # 4. Load State Dict (nơi thường xảy ra lỗi mismatch)\n",
    "                         print(\"--> Đang tải model_state_dict...\")\n",
    "                         try:\n",
    "                             model_to_evaluate.load_state_dict(checkpoint['model_state_dict'])\n",
    "                             print(\"Tải trọng số model thành công.\")\n",
    "                             model_to_evaluate.eval() # Chuyển sang chế độ đánh giá\n",
    "                             checkpoint_loaded_successfully = True # Đánh dấu thành công\n",
    "                         except RuntimeError as rte_load:\n",
    "                              print(\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "                              print(f\"LỖI RUNTIME khi load state_dict: {rte_load}\")\n",
    "                              print(\"===> Điều này có nghĩa kiến trúc model hiện tại KHÔNG KHỚP với checkpoint.\")\n",
    "                              print(\"===> Đảm bảo cell định nghĩa class CRNN và cell training đã dùng CÙNG MỘT cấu trúc CNN cắt bớt.\")\n",
    "                              print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "                         except KeyError as ke_load:\n",
    "                              print(f\"\\nLỗi KeyError khi load state_dict: {ke_load}. Checkpoint có thể bị lỗi.\")\n",
    "\n",
    "                else:\n",
    "                    print(\"Lỗi: Không tìm thấy 'char_to_int' trong checkpoint.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Lỗi: Không tìm thấy file checkpoint tại {load_path}.\")\n",
    "        except Exception as e_load_outer:\n",
    "            print(f\"Lỗi không xác định khi tải checkpoint: {e_load_outer}\")\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(f\"Lỗi: File checkpoint không tồn tại tại {load_path}.\")\n",
    "\n",
    "\n",
    "    # --- Chạy Đánh giá nếu mọi thứ sẵn sàng ---\n",
    "    if checkpoint_loaded_successfully and model_to_evaluate is not None and test_loader_htr is not None:\n",
    "        print(\"\\nBắt đầu chạy vòng lặp đánh giá trên test loader...\")\n",
    "        all_ground_truths = []\n",
    "        all_predictions = []\n",
    "        evaluation_progress_bar = tqdm(test_loader_htr, desc=\"Evaluating Test Set\", unit=\"batch\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_data_eval in evaluation_progress_bar:\n",
    "                # Bỏ qua batch lỗi từ collate_fn\n",
    "                if batch_data_eval is None or batch_data_eval[0] is None:\n",
    "                    continue\n",
    "\n",
    "                # Giải nén batch\n",
    "                images_eval, padded_labels_eval, _, label_lengths_eval = batch_data_eval\n",
    "                images_eval = images_eval.to(device)\n",
    "\n",
    "                try:\n",
    "                    # Dự đoán\n",
    "                    outputs_eval = model_to_evaluate(images_eval)\n",
    "\n",
    "                    # Giải mã Dự đoán (Greedy) - dùng map và blank index đã load\n",
    "                    predicted_texts = decode_batch(outputs_eval.cpu(), int_to_char_eval, blank_idx_eval)\n",
    "\n",
    "                    # Giải mã Nhãn gốc - dùng map đã load\n",
    "                    ground_truth_texts = decode_padded_labels(padded_labels_eval.cpu(), label_lengths_eval.cpu(), int_to_char_eval)\n",
    "\n",
    "                    # Lưu kết quả\n",
    "                    all_ground_truths.extend(ground_truth_texts)\n",
    "                    all_predictions.extend(predicted_texts)\n",
    "\n",
    "                except Exception as e_inf:\n",
    "                     print(f\"\\nLỗi trong quá trình inference hoặc decode batch: {e_inf}\")\n",
    "                     # Có thể in thêm thông tin batch để debug\n",
    "\n",
    "        # --- Tính toán WER và CER ---\n",
    "        print(\"\\nHoàn thành dự đoán. Bắt đầu tính toán WER/CER...\")\n",
    "        if not all_ground_truths or not all_predictions:\n",
    "             print(\"Lỗi: Không có kết quả dự đoán hoặc nhãn gốc để đánh giá.\")\n",
    "        elif len(all_ground_truths) != len(all_predictions):\n",
    "             print(f\"Lỗi: Số lượng nhãn gốc ({len(all_ground_truths)}) != dự đoán ({len(all_predictions)}).\")\n",
    "        else:\n",
    "            try:\n",
    "                # Áp dụng transform chuẩn hóa\n",
    "                transformed_gt = [jiwer_transform(s) for s in all_ground_truths]\n",
    "                transformed_pred = [jiwer_transform(s) for s in all_predictions]\n",
    "\n",
    "                # Tính toán\n",
    "                wer_score = jiwer.wer(transformed_gt, transformed_pred)\n",
    "                cer_score = jiwer.cer(transformed_gt, transformed_pred)\n",
    "\n",
    "                print(\"\\n===== Kết quả Đánh giá Cuối cùng =====\")\n",
    "                print(f\"Số mẫu đánh giá: {len(all_ground_truths)}\")\n",
    "                print(f\"Word Error Rate (WER): {wer_score:.4f}\")\n",
    "                print(f\"Character Error Rate (CER): {cer_score:.4f}\")\n",
    "\n",
    "                # In ví dụ\n",
    "                print(\"\\nVí dụ dự đoán:\")\n",
    "                num_examples = min(5, len(all_ground_truths))\n",
    "                for i in range(num_examples):\n",
    "                    print(\"-\" * 20)\n",
    "                    print(f\"GT [{i}]: {all_ground_truths[i]}\")\n",
    "                    print(f\"PD [{i}]: {all_predictions[i]}\")\n",
    "                print(\"-\" * 20)\n",
    "\n",
    "            except Exception as e_eval:\n",
    "                print(f\"\\nLỗi khi tính toán WER/CER bằng jiwer: {e_eval}\")\n",
    "\n",
    "    else:\n",
    "         print(\"\\nKhông thể thực hiện đánh giá: Model không được load thành công, hoặc test loader/char map không hợp lệ.\")\n",
    "\n",
    "# --- Dọn dẹp cuối cùng ---\n",
    "if 'model_to_evaluate' in locals(): del model_to_evaluate\n",
    "if 'checkpoint' in locals(): del checkpoint\n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n===== Script HTR hoàn tất =====\")"
   ],
   "id": "ec22df05512344f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Bắt đầu Đánh giá Cuối cùng trên Tập Test =====\n",
      "Đường dẫn checkpoint để đánh giá: G:\\Compvision_Final\\pythonProject\\checkpoint_htr_line_effnet_trunc_early.pth\n",
      "Đang tải checkpoint từ: G:\\Compvision_Final\\pythonProject\\checkpoint_htr_line_effnet_trunc_early.pth\n",
      "Tải file checkpoint thành công.\n",
      "-> Kiến trúc checkpoint khớp: 'EfficientNetB0_Truncated_Early_CRNN'\n",
      "-> Load thành công char_map (162 lớp, blank=0).\n",
      "--> Khởi tạo cấu trúc model CRNN...\n",
      "Đã xây dựng CNN bị cắt bớt SỚM HƠN từ EfficientNet-B0 pre-trained.\n",
      "Input size cho LSTM sẽ là: 24\n",
      "--- Khởi tạo CRNN với EfficientNet-B0 BỊ CẮT BỚT SỚM HƠN ---\n",
      "CNN Backbone: Truncated EfficientNet-B0 Features (ví dụ: [:3])\n",
      "CNN Output Channels (to RNN): 24\n",
      "CNN Output Width (SeqLen for RNN - đã tính): 96\n",
      "RNN Hidden Size: 256\n",
      "RNN Num Layers: 2\n",
      "Output Classes (incl. blank): 162\n",
      "------------------------------------------------------\n",
      "--> Đang tải model_state_dict...\n",
      "Tải trọng số model thành công.\n",
      "\n",
      "Bắt đầu chạy vòng lặp đánh giá trên test loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Test Set: 100%|██████████| 46/46 [00:15<00:00,  2.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hoàn thành dự đoán. Bắt đầu tính toán WER/CER...\n",
      "\n",
      "===== Kết quả Đánh giá Cuối cùng =====\n",
      "Số mẫu đánh giá: 1460\n",
      "Word Error Rate (WER): 0.9837\n",
      "Character Error Rate (CER): 0.5667\n",
      "\n",
      "Ví dụ dự đoán:\n",
      "--------------------\n",
      "GT [0]: với lập luận : nếu kéo dài thời gian không thu phí sẽ làm tăng lãi vay\n",
      "PD [0]: vi đôn điện. vn lhá chà đhngui không đhn h ti tàn đông ci ng\n",
      "--------------------\n",
      "GT [1]: Từ 12 g khuya 13 đến rạng sáng 14 họ đã tìm được xác 19 người. Đau\n",
      "PD [1]: T Nng hưngh ó đhế vng công ti thi đi bun hic vác t ngyưi, rm\n",
      "--------------------\n",
      "GT [2]: Sơn.\n",
      "PD [2]: .\n",
      "--------------------\n",
      "GT [3]: \" Mới bẫy đây! Các anh mua thịt lóc hay thịt xương? \". Chúng tôi hỏi\n",
      "PD [3]: Tgic côn đấn  nh m nhi đhi hng đh rông . hông đi hi\n",
      "--------------------\n",
      "GT [4]: xong quay ra đòi thỏa thuận lại...\n",
      "PD [4]: Tg n  đ t thn l.\n",
      "--------------------\n",
      "\n",
      "===== Script HTR hoàn tất =====\n"
     ]
    }
   ],
   "execution_count": 160
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
